---
title: "Supplemental_PRISMA_plot"
author: "Devi Veytia"
date: "2024-01-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r set up}

## Load libraries
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)
library(ggplot2)
library(metagear)
```


# PRISMA flow chart


```{r Get the latest version of sqlite database and connect}

sqliteDir <- here::here("data/sqlite-databases")
sqliteFiles <- dir(sqliteDir)
sqliteVersions <- as.numeric(gsub(".sqlite","",substring(sqliteFiles, regexpr("_v", sqliteFiles) + 2)))
latestVersion <- sqliteFiles[which.max(sqliteVersions)]
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, latestVersion), create=FALSE)
```

```{r summarise screened subset}
library(tidyverse)
seen_screen <- tbl(dbcon, "seen_screen")

manual_subset_summary <- seen_screen %>% 
  group_by(sample_screen, include_screen) %>%
  summarise(
    n = n_distinct(analysis_id)
  ) %>%
  tidyr::pivot_wider(names_from = include_screen, values_from = n) %>%
  collect()%>% 
  column_to_rownames(var = "sample_screen")

manual_subset_summary[is.na(manual_subset_summary)] = 0
manual_subset_summary["test list","0"] = 0

marginSums(as.matrix(manual_subset_summary),  c(1,2))
#                        0   1
# random              1656 427
# relevance sort       139 189
# supplemental coding    0 254
# test list              0  86
marginSums(as.matrix(manual_subset_summary),  c(2))
#    0    1 
# 1795  956

marginSums(as.matrix(manual_subset_summary),  c(1))
# random      relevance sort supplemental coding           test list 
#   2083                 328                 254                  86 

seen_coding <- tbl(dbcon, "seen_coding")


```


```{r summarise corpus}

# Connect to databases and get tables
allrefs <- tbl(dbcon,"allrefs") # all merged references in the corpus

dedups <- tbl(dbcon, "uniquerefs")  # unique references 

predRel <- tbl(dbcon, "pred_relevance")  # relevance predictions

# Summarise counts
databaseNum <- allrefs%>%
  group_by(source_database)%>% 
  summarise(n=n()) %>% 
  as.data.frame() # number from each source
naAbstracts <- allrefs%>%
  filter(is.na(abstract))%>% 
  summarise(n=n()) %>% 
  as.data.frame() # number of na Abstracts
nUnique <- as.data.frame(summarise(dedups, n=n())) # number of unique references
ndups <- sum(databaseNum$n)-naAbstracts-nUnique # number of duplicates removed
nInclude <- predRel %>% 
  summarise(
    lower = sum(0.5 <= relevance_lower),
    mean = sum(0.5 <= relevance_mean),
    upper = sum(0.5 <= relevance_upper)) %>%
  as.data.frame() # the number of articles predicted to be relevant

# disconnect  
dbDisconnect(dbcon)
```

```{r make prisma flow chart and write to figure}

PRISMAFlow <- c(
  paste("START_PHASE:", 
        format(databaseNum$n[databaseNum$source_database == "Web Of Science"], format = "d", big.mark=","),
        "articles from WOS"),
  paste("START_PHASE:", format(databaseNum$n[databaseNum$source_database == "Scopus"], 
                               format = "d", big.mark=","),
        "articles from Scopus"),
  paste(format(sum(databaseNum$n), format = "d", big.mark=","),"articles in total"),
  paste("EXCLUDE_PHASE:", format(naAbstracts, format = "d", big.mark=","),"abstracts removed as NA"),
  paste(format(sum(databaseNum$n)-naAbstracts, format = "d", big.mark=","),"eligible articles"),
  paste("EXCLUDE_PHASE:",
        format(sum(databaseNum$n)-naAbstracts-nUnique, format = "d", big.mark=","),"duplicates removed"),
  paste(format(nUnique, format = "d", big.mark=","),"unique articles"),
  paste("EXCLUDE_PHASE:", format(nUnique-nInclude["mean"], format = "d", big.mark=","),"articles excluded"),
  paste0(format(nInclude["mean"], format = "d", big.mark=",")," articles predicted relevant (+/- ", format(nInclude["mean"]-nInclude["lower"], format="d", big.mark=","),")")
)

w <- 12
h <- w*0.7

pdf(here::here("figures/supplemental/PRISMA_diagram.pdf"), width = w, height = h)
PRISMAFlowChart <- metagear::plot_PRISMA(PRISMAFlow, 
                                         design = c(E = "lightcoral", flatArrow = TRUE),
                                         excludeDistance = 0.8, colWidth = 50)
dev.off()

```


# Summarise manual screening and coding results


```{r manual screening summary}
seen_screen <- tbl(dbcon, "seen_screen") %>% collect()
seen_coding <- tbl(dbcon, "seen_coding") %>% collect()


# the number of inclusions vs exclusions
seen_screen %>%
  group_by(include_screen) %>%
  summarise(n = n())
#   include_screen     n
#            <int> <int>
# 1              0  1796
# 2              1   956
nrow(seen_screen) # in total, 2752

# the screening effort
screen_effort_summary <- seen_screen %>%
  summarise(
    DV = sum(grepl("devi", reviewer, ignore.case=TRUE)),
    JGP = sum(grepl("jean", reviewer, ignore.case=TRUE)),
    AC = sum(grepl("adrien", reviewer, ignore.case=TRUE))+sum(grepl("AC", reviewer)),
    LB = sum(grepl("bopp", reviewer, ignore.case=TRUE))+sum(grepl("Laurent", reviewer, ignore.case=TRUE)),
    YS = sum(grepl("yunne", reviewer, ignore.case=TRUE)),
    FV = sum(grepl("Fred", reviewer, ignore.case=TRUE))
  )
screen_effort_summary

#      DV   JGP    AC    LB    YS    FV
#   <int> <int> <int> <int> <int> <int>
# 1  1367   213   427   492   439    42

screen_effort_summary2 <- reshape2::melt(screen_effort_summary)

mean(screen_effort_summary2$value) # 496.6667
sd(screen_effort_summary2$value) # 458.8833
median(screen_effort_summary2$value) # 433


# Number of articles that were supplementally screened and coded using a keyword search
sum(seen_screen$sample_screen == "supplemental coding")
```







