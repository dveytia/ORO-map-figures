---
title: "0_data-processing-cleaning"
author: "Devi Veytia"
date: "2023-11-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries}
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)
library(countrycode)
```

# Add files to sqlite database

```{r Get the latest version of sqlite database}

sqliteDir <- here::here("data/sqlite-databases")
sqliteFiles <- dir(sqliteDir)
sqliteVersions <- as.numeric(gsub(".sqlite","",substring(sqliteFiles, regexpr("_v", sqliteFiles) + 2)))
latestVersion <- sqliteFiles[which.max(sqliteVersions)]


```



## Add geoparsing files

```{r add geoparsing results for all the geoparsed text}

# Connect to latest version of the database
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, latestVersion), create=FALSE)

# Get info about geoparsed text
geoparsedTextDir <- here::here("data/geoparsing/geoparsed-text")
geoparsedTextFiles <- dir(geoparsedTextDir)
tblName <- "geoparsed_text"

# Loop through csv files and write to database
for(i in 1:length(geoparsedTextFiles)){
  # Read in the csv file
  df <- readr::read_csv(file = file.path(geoparsedTextDir, geoparsedTextFiles[i]), show_col_types = FALSE)
  df <- df %>%
    rename(analysis_id = id)
  # If starting the table, don't need to append, if already there, then yes append
  if(i == 1){
    appendAction = FALSE
  }else{
    appendAction = TRUE
  }
  # Write to database
  dbWriteTable(conn=dbcon,name=tblName, value=df, append=appendAction, overwrite = FALSE)
}

DBI::dbDisconnect(dbcon) # DIsconnect
```

```{r add other geoparsing tables}
# Connect to latest version of the database
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, latestVersion), create=FALSE)

# Get a list of all the geoparsing file names
geoparsingDir <- here::here("data/geoparsing")
geoparsingFiles <- dir(geoparsingDir)
geoparsingFiles <- geoparsingFiles[grep(".csv", geoparsingFiles)]
geoparsingTableNames <- gsub(".csv","", geoparsingFiles)
geoparsingTableNames[-grep("grid_df_res", geoparsingTableNames)] <- gsub('[0-9]+', '', geoparsingTableNames[-grep("grid_df_res", geoparsingTableNames)])
geoparsingTableNames <- unique(geoparsingTableNames)

## Loop through all the tables and write to the database
for(i in 1:length(geoparsingTableNames)){
  # identify latest version of the file
  files <- geoparsingFiles[grep(geoparsingTableNames[i], geoparsingFiles)]
  fileVersions <- substr(files, nchar(geoparsingTableNames[i])+1, nchar(geoparsingTableNames[i])+1)
  if("." %in% fileVersions){
    fileVersions[which(fileVersions == ".")] <- 0
  }
  fileVersions <- as.numeric(fileVersions)
  file <- files[which.max(fileVersions)]
  
  # Read in file
  df <- readr::read_csv(file = file.path(geoparsingDir, file), show_col_types = FALSE)
  # Have to add this because some column names are duplicates (one is upper, the other is lower case)
  if(geoparsingTableNames[i] == "shp_df_natural-earth-shapes"){
    colnames(df) <- make.unique(tolower(colnames(df)), sep = "_")
  }
  # Write to database
  dbWriteTable(conn=dbcon, name=geoparsingTableNames[i], value=df, append=FALSE, overwrite = TRUE)
}

src_dbi(dbcon)

DBI::dbDisconnect(dbcon)
```


```{r additional data cleaning of false associations between country matches for v3}
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, "all_tables_v3.sqlite"), create=FALSE)

example_ids <- c(35143, 527)

geoparsed_text <- tbl(dbcon, "geoparsed_text") %>%
  filter(analysis_id %in% example_ids) %>%
  collect()

shp_df_matches <- tbl(dbcon, "geoparsed-text_shp_df_matches") %>%
  filter(analysis_id %in% example_ids) %>%
  collect()

grid_df <- tbl(dbcon, "grid_df_res2.5") %>%
  collect() 

grid_df2 <- grid_df %>% right_join(shp_df_matches, by = "grid_df_id")


## quick plot
plot(grid_df$LON, grid_df$LAT)

# So the geoparsing and matching to shapefiles looks fine, but its the intersection between shpfile polygons to grid ids that looks to be the problem. Try in R? The relevant shp_id is 31



##  Recalculate shp_id to grid_df_id matching in R -----------
shp_grid_df <- read.csv("C:\\Users\\deviv\\R-working-folder\\spacy_example\\outputs\\shp_grid_df.csv")

countries <- map_data("world")

shp_grid_df %>%
  filter(shpfile_id == 31) %>%
  left_join(grid_df, by = "grid_df_id") %>%
  View() 

shp_grid_df %>%
  filter(shpfile_id == 31) %>%
  left_join(grid_df, by = "grid_df_id") %>%
  ggplot(aes(LON, LAT))+
  geom_point(col="red")+
  geom_polygon(data = countries, aes(long, lat, group=group))

# The version calculated in my spacy script seems good, so use this and make the sqlite file version 2

shp_df_matches <- tbl(dbcon, "geoparsed-text_shp_df_matches") %>% collect()

# Separate where there is and isn't a shp_id
shp_df_matches_noID <- shp_df_matches %>% filter(is.na(shp_id))
shp_df_matches_ID <- shp_df_matches %>% filter(!is.na(shp_id))

# Get unique combinations of analysis id, shp id, and place to rematch to grid cells
shp_df_matches_ID_unique <- shp_df_matches_ID[c("analysis_id","shp_id","place")]
shp_df_matches_ID_unique <- shp_df_matches_ID_unique[!duplicated(shp_df_matches_ID_unique),]

# for each shp_id, add new rows for all the relevant grid cells
shp_df_matches_ID_unique <- merge(shp_df_matches_ID_unique, shp_grid_df %>% rename(shp_id = "shpfile_id"),
                                  by = "shp_id", all.x = TRUE, all.y = FALSE)


# Double check again shp_id 31 (the UK) -- yes
shp_df_matches_ID_unique %>%
  filter(shp_id == 31) %>%
  left_join(grid_df, by = "grid_df_id") %>%
  ggplot(aes(LON, LAT))+
  geom_point(col="red")+
  geom_polygon(data = countries, aes(long, lat, group=group))



# Calculate the grid cell weight
## For each unique document and place, calculate the 1/the number of grid cells occupied by that place to get the grid cell weight
shp_df_matches_ID_unique <- shp_df_matches_ID_unique %>%
  group_by(analysis_id, shp_id) %>%
  mutate(cell_weight = 1/n())

# noticed that where there was no shp_id, cell weight was NA so recalculate grouping by place
shp_df_matches_noID <- shp_df_matches_noID %>%
  group_by(analysis_id, place) %>%
  mutate(cell_weight = 1/n())

## Rejoin new shp_id to grid matches with those rows that didn't have a shp_id
shp_df_matches2 <- rbind(shp_df_matches_ID_unique, shp_df_matches_noID)


## Recalculate grid sums
shp_df_sum = shp_df_matches2 %>%
  group_by(grid_df_id) %>%
  summarise(n_articles = n(), n_articles_weighted = sum(cell_weight, na.rm=TRUE)) %>%
  right_join(grid_df, by = "grid_df_id")

# quick plot
shp_df_sum %>%
  ggplot()+
  geom_tile(aes(LON, LAT, fill = log(n_articles_weighted)))+
  scale_fill_viridis_c(option = "D")+
  geom_polygon(data = countries, aes(long, lat, group=group), fill="transparent")
  #coord_map(projection = "mercator")


DBI::dbDisconnect(dbcon)

```

```{r Add fixes to new sqlite database v4}
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, "all_tables_v4.sqlite"), create=FALSE)

# Write over old tables
dbWriteTable(conn=dbcon, name="geoparsed-text_shp_df_matches", value=shp_df_matches2, append=FALSE, overwrite = TRUE)

dbWriteTable(conn=dbcon, name="shp_grid_df", value=shp_grid_df, append=FALSE, overwrite = TRUE)

dbWriteTable(conn=dbcon, name="geoparsed-text_grid-sums", value=shp_df_sum, append=FALSE, overwrite = TRUE)


DBI::dbDisconnect(dbcon)
```


## Clean name matching errors - v5

This section will clean the bad place matches from the geoparsing tables.

Steps to clean:
0. Select only empirical papers
0.bis. Correct shp_id identification with the file called geoparsing_wordPlaceMatches95quantile_cleaned
1. Select only data were a the place name matches a shapefile ID
2. Remove acronym matches where the word matched is all capitalized letters, except UK and US
3. Remove text between copyright symbol and next period (get rid of publisher matching)
4. Remove land matches x distance from the coast when the spatial scale of the match is sub-national
<!-- 5. Remove affiliation text from abstract, and any resulting word matches -->

6. Remove word matches that are a complete word in the English language
7. Weight match by country_conf value (0.6 - 1)** -> consider removing low values using cluster analysis altogether


Notes on no. 4: 
There are several definitions we can use to identify coastal land (from least to most conservative):
a. Within 100 km from the land-sea interface (WRI 2000,Burke et al. 2001)
b. Landward area contained within 100 m elevation of sea level (Nicholls and Small 2002)
c. Landward area contained within the 200 m land elevation contour ((Land-ocean interaction in the coastal zone programme -- now Future Earth Coasts)[https://www.researchgate.net/publication/226867603_The_Coastal_Zone_-_a_Domain_of_Global_Interactions], Pernettaand Milliman 1995). This is designed to capture the area "extending from the coastal plains to the outer edge of the continental shelves, approximately matching the region that has been alternatively flooded and exposed during the sea level fluctuations of the late Quaternary period‚Äù(Holligan and de Boois 1993). 

I suggest to take the conservative approach of option "c" and do landward area within 200 m land elevation contour. That said, if it is too complicated/time consuming, then option "a" looks easier -- we could make the argument that we extend it to 200 km from coast because given sea level rise locations further inland may be concerned with coastal research? 

*Estimating error rate*

Another approach is to create a lookup table of unique word and place_name matches and double check manually -- this can then be merged into the geoparsed_text table to indicate whether a match is TRUE, FALSE, or ambiguous. This will allow us to estimate the likely error rate, which can be stratified by country. This information can be used to:
a) clean the data by completely eliminating matches that are FALSE
b) estimate "uncertainty" for each country using the "ambiguous" classification -- perhaps this information can be displayed alongside the data plotted in the map via stippling etc. This will likely apply to countries like the USA, where records like "south beach" are attributed.



```{r load data}
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, "all_tables_v4.sqlite"), create=FALSE)

geoparsedText <- tbl(dbcon, "geoparsed_text") 
shp_df_matches <- tbl(dbcon, "geoparsed-text_shp_df_matches") 
shp_df_sum <- tbl(dbcon, "geoparsed-text_grid-sums")
uniquerefs <- tbl(dbcon, "uniquerefs")
grid_df <- tbl(dbcon, "grid_df_res2.5") %>% collect() 
```


```{r steps for cleaning, include=FALSE}

### Global geoparsed data
geoP_data <- tbl(dbcon, "geoparsed_text") |>  
    dplyr::select(analysis_id, word, country_predicted, place_name) |> 
    collect() 

nrow(geoP_data)
length(unique(geoP_data$analysis_id))


### Note N¬∞0: Select only empirical studies

  ## Extract the papers predicted as empirical ones
  predMethod <- tbl(dbcon, "pred_method_type") |> 
    select(analysis_id, 
           `method_type.Empirical - mean_prediction`) |> 
    filter(`method_type.Empirical - mean_prediction` >= 0.5) |> 
    collect()
  
  ## Selection of empiral papers among all papers
  geoP_data_empirical <- filter(geoP_data, analysis_id %in% unique(predMethod$analysis_id))
  
  nrow(geoP_data_empirical)
  length(unique(geoP_data_empirical$analysis_id))
  
### Note N¬∞1: Select only data where !is.na()
  
  ## Join with the geoparsed matching location (grid cell level)
  geoP_data_empirical_cell <- geoP_data_empirical |> 
    left_join(shp_df_matches, by = "analysis_id", copy = TRUE) |>
    collect()
  
  nrow(geoP_data_empirical_cell)
  length(unique(geoP_data_empirical_cell$analysis_id))
  
  ## Filter only rows where there the shp_id column != NA
  geoP_data_empirical_cell_noNA <- filter(geoP_data_empirical_cell, !is.na(shp_id))
  nrow(geoP_data_empirical_cell_noNA)
  length(unique(geoP_data_empirical_cell_noNA$analysis_id))
  
  
### Note N¬∞2: Capital letter filtering
  
  # (Filter row that are not in capital letter) OR
  # (Filter row in capital letters & with a number of character > 3) OR ~ to avoid acronyme but keep city name in capital letters
  # (Filter row in capital letters & matching with USA, UK or US) ~ to avoid loosing data on these 2 countries
  geoP_data_empirical_cell_acronym <- geoP_data_empirical_cell |> 
    filter((stringr::str_detect(word, pattern = "^[[:upper:][:space:]]+$") == FALSE) |
           (stringr::str_detect(word, pattern = "^[[:upper:][:space:]]+$") == TRUE & nchar(word) > 3) |
           (stringr::str_detect(word, pattern = "^[[:upper:][:space:]]+$") == TRUE & word %in% c("USA", "UK", "US")))
  
  nrow(geoP_data_empirical_cell_acronym)
  length(unique(geoP_data_empirical_cell_acronym$analysis_id))

  
### Note N¬∞3: Remove word matching with copyright informations
  
  ## Extract analysis_id that have (c) symbole in the abstract
  geoparsedText_with_C <- uniquerefs |>
    collect() |> 
    # Filter only analysis_id keept at previous filter
    filter(analysis_id %in% unique(geoP_data_empirical_cell_acronym$analysis_id)) |> 
    # Filter only analysis_id with the copyright symbole
    filter(stringr::str_detect(abstract, pattern = "√Ç¬©"))
    # mutate(copyright_text = stringr::str_extract(abstract, "\\√Ç¬©\\w+$"))
    
   for(i in 1:nrow(geoparsedText_with_C)){
     
     geoparsedText_with_C$copyright_text[i] <- stringr::str_split(geoparsedText_with_C$abstract[i], "√Ç¬©")[[1]][2]
     
   } 
  
  geoparsedText_with_C <- geoparsedText_with_C |> 
    dplyr::select(analysis_id, copyright_text)
  
  ## Filter data with "word" matching with a name in "copyright_text
  geoP_data_empirical_cell_acronym_CR <- geoP_data_empirical_cell_acronym |> 
    left_join(geoparsedText_with_C, by = "analysis_id") |> 
    mutate(filter = case_when(stringr::str_detect(copyright_text, stringr::fixed(word)) ~ TRUE,
                              TRUE ~ FALSE)) |> 
    filter(filter == FALSE) |> 
    dplyr::select(-copyright_text, -filter)
  
  nrow(geoP_data_empirical_cell_acronym_CR)
  length(unique(geoP_data_empirical_cell_acronym_CR$analysis_id))
  
  
### Note N¬∞4: Remove land matches x distance from the coast when the spatial scale of the match is sub-national
  
  ## Set new grid dataframe with new info
  grid_df2 <- grid_df
  
  ## Within the EEZ----------------------------
  require(raster)
  eez_rast <- raster::raster(here::here("data", "external", "eez_rast", "rast_iso_eez.asc"))
  eez_rast[eez_rast == 0] <- NA
  
    # extract eez value at grid cell points
    grid_df2$eez_vals <- raster::extract(eez_rast, grid_df[,c("LON","LAT")])
    
  ## Distiance inlad from the coast 
    
    # Used this as inspo: https://dominicroye.github.io/en/2019/calculating-the-distance-to-the-sea-in-r/
    coast <- giscoR::gisco_get_countries(
      spatialtype = "COASTL",
      epsg = "4326"
    )
    
    coast_utm <- sf::st_transform(coast, 4087)
  
    landCells_sf <- grid_df |>  
      dplyr::filter(is_land == 1) |> 
      sf::st_as_sf(coords = c('LON','LAT')) |> 
      sf::st_set_crs(4326) |> 
      sf::st_transform(4087)

    #transform from polygon shape to line
    coast_utm <- sf::st_cast(coast_utm, "MULTILINESTRING")
    
    #calculation of the distance between the coast and our points
    dist <- sf::st_distance(coast_utm, landCells_sf)
    
    # will need to compute the minimum distance for each land cell
    minDist <- apply(dist, 2, function(x) min(x, na.rm=T))
    minDist <- minDist/1000 # convert to km
    landCells_sf$distInLand_km <- minDist
    landCells <- as.data.frame(landCells_sf)
    
    grid_df2 <- grid_df2 |> 
      dplyr::left_join(landCells |>  dplyr::select(grid_df_id, distInLand_km))
    
    # Plot to check -- this works!
    ggplot(grid_df2) +
      geom_point(aes(LON, LAT, col = distInLand_km))
    

  ## Make a new column to identify inland or coast
  grid_df2 <- grid_df2 %>%
    mutate(coastal = ifelse(distInLand_km <= 200 | !is.na(eez_vals), 1, 0))
  
    # check
    grid_df2 %>%
      ggplot(aes(LON, LAT, fill=coastal))+
      geom_tile()
    
  ## Filter only cells that are at least 200km from coast
  geoP_data_empirical_cell_acronym_CR_200km <- geoP_data_empirical_cell_acronym_CR |> 
    left_join(grid_df2 |> dplyr::select(grid_df_id, coastal), by = "grid_df_id") |> 
    filter(coastal == 1) |> 
    dplyr::select(-coastal)
    
  nrow(geoP_data_empirical_cell_acronym_CR_200km)
  length(unique(geoP_data_empirical_cell_acronym_CR_200km$analysis_id))
  
### Note N¬∞0.bis: Correction of mis-identified shp_id
  
  ## Grid of shapefile and matching cells
  load(here::here("data", "geoparsing", "land_eez_grid_country.RData"))
  shp_df_matches_grid <- shp_df_matches |> 
    collect() |> 
    distinct(shp_id, grid_df_id) |> 
    left_join(grid_df_eez_land |>  dplyr::select(-LAT, -LON), by = "grid_df_id") |> 
    filter(!is.na(shp_id))
 
  ## Join with the geoparsed matching location (grid cell level)
  # geoP_data_empirical_cell <- geoP_data_empirical |> 
  #   left_join(shp_df_matches, by = "analysis_id", copy = TRUE) |>
  #   collect()
  
  ## Load the corrected table of shp_id
  geoparsing_wordPlaceMatches95quantile_cleaned <- readr::read_csv(here::here("data", "geoparsing", "geoparsing_wordPlaceMatches95quantile_cleaned.csv")) |>
    filter(true_match == FALSE & nchar(shp_id_correct) < 5)
  
  ## For loop to correct all results 
  geoP_data_empirical_cell_acronym_CR_200km_Q95 <- geoP_data_empirical_cell_acronym_CR_200km
  for(i in 1:nrow(geoparsing_wordPlaceMatches95quantile_cleaned)){
    
    print(i)
    
    # Extract the correct shp_id 
    shp_id_correct <- geoparsing_wordPlaceMatches95quantile_cleaned$shp_id_correct[i]
    
    # Extract the wrong word
    wrong_word <- geoparsing_wordPlaceMatches95quantile_cleaned$word[i]
    
    # Extract the wrong place_name
    wrong_place_name <- geoparsing_wordPlaceMatches95quantile_cleaned$place_name[i]
    
    # tmp true grid 
    shp_df_matches_grid_i <- shp_df_matches_grid |> 
      filter(shp_id == shp_id_correct) |> 
      distinct(shp_id, TERRITORY1, country_id, .keep_all = TRUE) # Keep one grid cell

    # Extract and correct data in the main table
    data_corr <- geoP_data_empirical_cell_acronym_CR_200km |> 
      # filter(analysis_id == 348090) |> 
      filter(word == wrong_word & place_name == wrong_place_name) |> 
      dplyr::select(-grid_df_id) |>
      mutate(shp_id = as.numeric(shp_id_correct)) |> 
      left_join(shp_df_matches_grid_i, by = "shp_id") |> 
      filter(word == TERRITORY1 | word == country_id) |> 
      distinct(analysis_id, grid_df_id, TERRITORY1, country_id, .keep_all = T) |> 
      dplyr::select(names(geoP_data_empirical_cell_acronym_CR_200km)) 
    
    
  ## Replace wrong data by the true ones
  geoP_data_empirical_cell_acronym_CR_200km_Q95 <- geoP_data_empirical_cell_acronym_CR_200km_Q95 |> 
    filter(!(word == wrong_word & place_name == wrong_place_name)) |> # word != wrong_word & place_name != wrong_place_name
    rbind(data_corr)
  
  # print(sum(geoP_data_empirical_cell_acronym_CR_200km_Q95$analysis_id == 348090))
    
  } # end FOR i
  
  nrow(geoP_data_empirical_cell_acronym_CR_200km_Q95)
  length(unique(geoP_data_empirical_cell_acronym_CR_200km_Q95$analysis_id))

  
### Note 6: Remove word matches that are a complete word in the English language
library(qdapDictionaries)
library(maps)
library(datasets)

  # Identify rows with "word" being an english word
  is.word  <- function(x) x %in% GradyAugmented
  is.city <- function(x) x %in% as.data.frame(world.cities)$name
  is.US.state <- function(x) x %in% state.name

  geoP_data_empirical_cell_acronym_CR_200km_Q95_word <- geoP_data_empirical_cell_acronym_CR_200km_Q95 |> 
    mutate(is_word = is.word(stringr::str_to_lower(word)),
           country_word = countrycode::countrycode(sourcevar   = word,
                                                   origin      = "country.name",
                                                   destination = "country.name"),
           is_city = is.city(stringr::str_to_title(word)),
           is_USstate = is.US.state(stringr::str_to_title(word))) |> 
    # Keep rows that are not word OR rows that are word but a country, city or US state
    filter((is_word == FALSE) | (is_word == TRUE & !is.na(country_word)) | (is_word == TRUE & is_city == TRUE) | (is_word == TRUE  & is_USstate == TRUE))
  
  nrow(geoP_data_empirical_cell_acronym_CR_200km_Q95_word)
  length(unique(geoP_data_empirical_cell_acronym_CR_200km_Q95_word$analysis_id))
  
### Note +1: Remove word matches that contains a number or that looks like species scientific name
  
  ## Filter all rows matching a species name like pattern
  #     ^: Asserts the start of the string.
  # [A-Z]: Matches any uppercase letter from A to Z.
  # \.: Matches a period (dot) character literally.
  # \s: Matches an space character (zero or one space).
  # [a-zA-Z]+: Matches one or more alphabetic characters (both lowercase and uppercase). This ensures that there is at least one letter following the period.
  # (\\s[a-zA-Z]+)?: This is an optional group. It starts with a space (\s) followed by one or more alphabetic characters for the third word. The ? quantifier   makes the entire group optional, meaning the third word is optional.
  # $: Asserts the end of the string.
  geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num <- geoP_data_empirical_cell_acronym_CR_200km_Q95_word |> 
    filter(!stringr::str_detect(word, ".*[0-9].*") & !stringr::str_detect(word, "^[A-Z]\\.\\s[a-zA-Z]+(\\s[a-zA-Z]+)?$")) |> 
    # Manual filter for genus names 
    filter(! word %in% c("Spartina", "Avicennia", "avicennia", "Posidonia", "Saccharina", "Casuarina", "Saltmarshes", "saltmarshes", "Offshore Wind Farms", "Mangrove Forests", "sandy beach", "sandy beaches", "anthozoa", "sea water", "a Coral Reef", "- bay marsh", "a North Sea", "Corg", "Kandelia", "-bay marsh", "(South Pacific", "a Rhode Island", "west coast", "East Coast", "east coast", "West Coast", "s Pacific coast", "West coast", "East coast", "Mangrove Coast", "Nature Coast", "Northwest Coast", "sandy coast", "south coast", "Southern Coast", "Western Coast", "Westcoast", "Coastal dunes", "monopile", "marina", "Mobile", "Chelonia", "Codium", "	
Phragmites", "amphipoda", "non-u.s.", "Puffinus", "Typha", "Douglas-fir", "states√¢‚Ç¨‚Ñ¢", "chiroptera", "J.", "Reef Ball√¢‚Äû¬¢", "salinas", "Savannah", "Sea Changes", "ano√¢‚Ç¨‚Ñ¢", "C.J.U", "√ß¬°¬´√¶", "LNG RV", "llc", "lng terminals", "copepoda", "coral reef island", "d. peresta", "DC/DC", "Eocene", "ESTUARINE", "et al", "far east", "Far East", "FLNG", "Fungia", "GNSS", "GoA", "helianthus", "Heliconia", "HERIOT-WATT", "High Island", "√é‚ÄùpHNBS", "√é¬±-carbonic",
"√é¬ºg", "j.a.l.", "K+", "Jq", "kgC", "km coastal", "Marina", "Masui et al.", "Mg", "mhk", "milkfish ponds", "middle marsh", "Mn", "N-m", "nddc", "NDVI", "Nitzschia", "No Fishery", "	OC-OTEC", "Ocean Waves", "Ocean Station", "Ocean Worlds", "oxic", "Parcel F", "Pb", "Peatlands", "PETRONAS", "Pinus", "Plover Cove (", "Porites", "POST-ON", "RAMSAR", "Ramsar", "reef lagoon", "river valley", "salt marshes", "Salt Marshes", "Salt Ponds", "sCOD", "SE North Sea", "Sea Change", "sea coast", "silt+clays", "site M", "soc", "SoC", "sub sea", "Sylt", "Symbioses", "t.nguyen", "Trelleborg√¢‚Ç¨‚Ñ¢s", "Undaria", "X Field", "MeyGen", "Sandy beaches", "Low earth", "Coastal Waters", "Wave Hub", "L Parr")) |>
    filter(! place %in% c("sp√°rtina")) 

  nrow(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num)
  length(unique(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num$analysis_id))
  
  
### Note +2: Remove rows when the grid cell falls in a country without border with the corresponding "word" when it is a sea or an ocean
### Example with word == "North Sea"
load(here::here("data", "geoparsing", "land_eez_grid_country.RData"))

  ## Load country and ocean/sea shapefiles
  countries <- sf::st_read(here::here("data", "external", "country", "ne_50m_admin_0_countries.shp"))  # Update with correct path
  oceans_seas <- sf::st_read(here::here("data", "external", "ocean", "ne_50m_geography_marine_polys.shp"))  # Update with correct path
  
  ## Identify countries having a border with a given sea/ocean
  sf::sf_use_s2(FALSE)
  oceans_seas_intersect <- sf::st_intersection(countries, oceans_seas) |> 
    sf::st_drop_geometry() |> 
    dplyr::select(name_en, SOVEREIGNT, ADMIN) |> 
    dplyr::rename(ocean_sea     = name_en,
                  country_sea   = SOVEREIGNT,
                  territory_sea = ADMIN) |> 
    mutate(country_id = countrycode::countrycode(sourcevar   = country_sea,
                                                 origin      = "country.name",
                                                 destination = "country.name"))
  
  oceans_seas_intersect$ocean_sea[oceans_seas_intersect$ocean_sea == "Gulf of Lion"] <- "Lion's Gulf"
  
  ## Format data
  geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea <- geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num |>  
    dplyr::select(analysis_id, word, shp_id, place_name, grid_df_id, place) |>
    left_join(grid_df_eez_land |>  dplyr::select(-LAT, -LON), by = "grid_df_id") 
  
    # Select only data that need a check
    data_test_sea_tmp <- geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea |> 
      filter(stringr::str_to_title(word) %in% oceans_seas_intersect$ocean_sea) 

    data_to_keep <- geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea |> 
      filter(! stringr::str_to_title(word) %in% oceans_seas_intersect$ocean_sea)
    
  ## Remove rows when a grid cell falls in a country without any border with the corresponding "word" when it is a sea or an ocean
    
    # For loop for dectection
    for(i in 1:nrow(data_test_sea_tmp)){
      
      # Row selection
      row_i <- data_test_sea_tmp[i,]
      
      # If match a name in oceans_seas, make the check (and keep or remove the row), ELSE, keep the row
      matching_sea <- stringr::str_to_title(row_i$word)
      
      if(row_i$country_id %in% oceans_seas_intersect$country_id[oceans_seas_intersect$ocean_sea == matching_sea]){
        data_test_sea_tmp$keep[i] <- "yes"
      } else {data_test_sea_tmp$keep[i] <- "no"}
      
    } ; sum(data_test_sea_tmp$keep == "no")
    
    # Keep only true data
    geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea <- data_test_sea_tmp |> 
      filter(keep == "yes") |> 
      dplyr::select(-keep) |> 
      rbind(data_to_keep)
  
  nrow(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea)
  length(unique(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea$analysis_id))
  
### Note +3: For analysis_id with a country name in the title, keep only the country in the title (avoid scale mistakes and comparisons)
### Example: analysis_id == 264937
  
  ## Identify unique analysis_id
  ana_id_unique <- unique(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea$analysis_id)
  geopparsedText_sel <- geoparsedText |> dplyr::select(analysis_id, title) |>  distinct() |>  collect()
  
  ## For loop to check for each paper if there is a country in the title
  geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title <- NULL
  drop_data <- NULL
  keep_data <- NULL
  for(i in 1:length(ana_id_unique)){
    
    # # Select all row for the ith analysis_id
    # analysis_id_i <- geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea |> 
    #   filter(analysis_id == ana_id_unique[i]) |> 
    #   left_join(geopparsedText_sel, by = "analysis_id", copy = TRUE) |> 
    #   mutate(country_in_title = stringr::str_detect(title, TERRITORY1))
    # 
    # # Keep only TRUE data if country nale identified in the title, else, keep all data 
    # if(sum(analysis_id_i$country_in_title == TRUE) > 0){
    #   keep_data <- analysis_id_i |> filter(country_in_title == TRUE) |> dplyr::select(-country_in_title, -title)
    #   drop_data <- rbind(drop_data, analysis_id_i |> filter(country_in_title == FALSE))
    # } else {keep_data <- analysis_id_i |> dplyr::select(-country_in_title, -title)}
    
    # Select all row for the ith analysis_id
    analysis_id_i <- geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea |> 
      filter(analysis_id == ana_id_unique[i]) |> 
      left_join(geopparsedText_sel, by = "analysis_id", copy = TRUE)
    
    # Replace abbreviations by country names
    if(sum(stringr::str_detect(analysis_id_i$title, c("UK|USA|US"))) > 0){
      
      abbrev_i <- unique(stringr::str_extract(analysis_id_i$title, c("UK|USA|US")))
      country_i <- countrycode::countrycode(sourcevar   = abbrev_i,
                                            origin      = "country.name",
                                            destination = "country.name")
      analysis_id_i <- analysis_id_i |> 
        mutate(title = stringr::str_replace(title, abbrev_i, country_i))
      
    }
    
    # Keep only TRUE data if country nale identified in the title, else, keep all data
    analysis_id_i <- analysis_id_i |>  
      mutate(country_in_title = stringr::str_detect(title, TERRITORY1) | stringr::str_detect(title, c("UK|USA|US")))
    
    if(sum(analysis_id_i$country_in_title == TRUE) > 0){
      keep_data <- analysis_id_i |> filter(country_in_title == TRUE) |> dplyr::select(-country_in_title, -title)
      drop_data <- rbind(drop_data, analysis_id_i |> filter(country_in_title == FALSE))
    } else {keep_data <- analysis_id_i |> dplyr::select(-country_in_title, -title)}
    
    geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title <- rbind(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title, keep_data)
    
  }

  nrow(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title)
  length(unique(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title$analysis_id))
  # save(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title, file = here::here("data", "geoparsing", "tmp_clean.RData"))
  
### Note +4: For analysis_id with a country name in the key_words, keep only the country in the key words (avoid scale mistakes and comparisons)
### Example: analysis_id == 373172, 288215, 269903,335868
  
  ## Identify unique analysis_id
  ana_id_unique <- unique(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title$analysis_id)
  uniquerefs_kw <- uniquerefs |> dplyr::select(analysis_id, keywords, keywords_other) |> filter(analysis_id %in% ana_id_unique) |>  distinct() |>  collect()
  
  ## Keep sea and gulf having a country names
  sea_with_country_name <- oceans_seas_intersect |> 
    filter(stringr::str_detect(stringr::str_to_lower(ocean_sea), stringr::str_to_lower(country_sea))) 
  
  sea_with_country_name <- unique(sea_with_country_name$ocean_sea)
  
  ## For loop to check for each paper if there is a country in the title
  geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title_KW <- NULL
  drop_data_KW <- NULL
  for(i in 1:length(ana_id_unique)){
    
    # print(i)
    
    analysis_id_i <- geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title |> 
      filter(analysis_id == ana_id_unique[i]) |> 
      left_join(uniquerefs_kw, by = "analysis_id", copy = TRUE)
    
    # Replace abbreviations by country names
    if(sum(stringr::str_detect(analysis_id_i$keywords, c(", uk|; uk|, usa|; usa|/usa/")), na.rm = TRUE) > 0){
      
      abbrev_i <- stringr::str_remove(unique(stringr::str_extract(analysis_id_i$keywords, c(", uk|; uk|, usa|; usa|/usa/"))), c(", |; |/"))
      country_i <- countrycode::countrycode(sourcevar   = abbrev_i,
                                            origin      = "country.name",
                                            destination = "country.name")
      analysis_id_i <- analysis_id_i |> 
        mutate(keywords = stringr::str_replace(keywords, abbrev_i, country_i))
      
    }
    
    # print(i)
    
    # Select all row for the ith analysis_id
    # analysis_id_i <- geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title |> 
    #   filter(analysis_id == ana_id_unique[i]) |> 
    #   left_join(uniquerefs_kw, by = "analysis_id", copy = TRUE) |> 
    #   mutate(country_in_kw  = stringr::str_detect(keywords, stringr::str_to_lower(TERRITORY1)),
    #          country_in_kwo = stringr::str_detect(keywords_other, stringr::str_to_lower(TERRITORY1)),
    #          US_state_in_kw = case_when(stringr::str_detect(keywords, paste(stringr::str_to_lower(state.name), collapse = "|")) == TRUE & TERRITORY1 == "United States" ~ TRUE, 
    #                                     TRUE ~ FALSE)) |>
    #   tidyr::replace_na(list(country_in_kw = FALSE, country_in_kwo = FALSE, US_state_in_kw = FALSE))
    analysis_id_i <- suppressWarnings(analysis_id_i |> 
      mutate(country_in_kw  = case_when(stringr::str_detect(keywords, stringr::str_to_lower(TERRITORY1)) ~ TRUE,
                                        (!stringr::str_detect(keywords, stringr::str_to_lower(TERRITORY1)) & sum(stringr::str_detect(keywords, stringr::str_to_lower(sea_with_country_name)) == TRUE, na.rm=T) > 0) ~ TRUE,
                                        TRUE ~ FALSE),         
             country_in_kwo = case_when(stringr::str_detect(keywords_other, stringr::str_to_lower(TERRITORY1)) ~ TRUE,
                                        (!stringr::str_detect(keywords_other, stringr::str_to_lower(TERRITORY1)) & sum(stringr::str_detect(keywords_other, stringr::str_to_lower(sea_with_country_name)) == TRUE, na.rm=T) > 0) ~ TRUE,
                                        TRUE ~ FALSE),
             US_state_in_kw = case_when(stringr::str_detect(keywords, paste(stringr::str_to_lower(state.name), collapse = "|")) == TRUE & TERRITORY1 == "United States" ~ TRUE, 
                                        TRUE ~ FALSE)) |>
  tidyr::replace_na(list(country_in_kw = FALSE, country_in_kwo = FALSE, US_state_in_kw = FALSE)))
    
    # Keep only TRUE data if country nale identified in the title, else, keep all data 
    if(sum(analysis_id_i$country_in_kw == TRUE) > 0 | sum(analysis_id_i$country_in_kwo == TRUE) > 0 | sum(analysis_id_i$US_state_in_kw == TRUE) > 0){
      keep_data <- analysis_id_i |> filter(country_in_kw == TRUE | country_in_kwo == TRUE | US_state_in_kw == TRUE) |> 
        dplyr::select(-country_in_kw, -country_in_kwo, -US_state_in_kw, -keywords, -keywords_other)
      drop_data_KW <- rbind(drop_data_KW, analysis_id_i |> filter(country_in_kw == FALSE))
    } else {keep_data <- analysis_id_i |> dplyr::select(-country_in_kw, -country_in_kwo, -US_state_in_kw, -keywords, -keywords_other)}
    
    geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title_KW <- rbind(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title_KW, keep_data)
    
  }

  nrow(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title_KW)
  length(unique(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title_KW$analysis_id))
  
  # tmp <- geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title_KW |> 
  #   distinct(analysis_id, TERRITORY1, country_id)
  # 
  # drop_tmp <- drop_data_KW |> 
  #   distinct(analysis_id, word, TERRITORY1, country_id, keywords)

### Remove High sea cells  
geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title_KW_hs <- geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title_KW |> 
  filter(country_id != "High-seas")

nrow(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title_KW_hs)
length(unique(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title_KW_hs$analysis_id))

save(geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title_KW_hs, file = here::here("data", "geoparsing", "tmp_clean.RData"))

test_verif_per_of_true <- geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title_KW_hs |> 
  distinct(analysis_id, word, TERRITORY1, country_id)

select_pubs <- unique(test_verif_per_of_true$analysis_id) ; sample_pubs <- sample(select_pubs, 100)

x <- 100
tmp <- test_verif_per_of_true |>  filter(analysis_id == sample_pubs[x]) ; sample_pubs[x] ; unique(tmp$country_id)
testX <- uniquerefs |> 
  collect() |> 
  filter(analysis_id == sample_pubs[x]) ; testX$doi ; testX$title ; testX$keywords # ; testX$abstract 

```


```{r visualize data structure}
# uniqueWordPlaceMatchesCountryPred <- geoparsedText %>% select(word, place_name) %>% distinct(word, place_name) 
load(here::here("data", "geoparsing", "land_eez_grid_country.RData"))

uniqueWordPlaceMatchesCountryPred <- geoP_data_empirical_cell_acronym_CR_200km_Q95_word_num_sea_title_KW_hs |>  
  dplyr::select(word, shp_id, place_name, grid_df_id, analysis_id, place) |> 
  # geoparsedText |> 
  # collect() |> 
  # filter(analysis_id %in% geoP_data_empirical_cell_acronym_CR_200km_word_num$analysis_id) |>
  # dplyr::select(-cell_weight, -coastal, -is_word, -is_city, -is_USstate) |> 
  # left_join(shp_df_matches, by = "analysis_id", copy = TRUE) |>
  # right_join(geoP_data_empirical_cell_acronym_CR_200km_word_num |>  dplyr::select(shp_id, grid_df_id, analysis_id, place), by = "analysis_id") |>
  left_join(grid_df_eez_land |>  dplyr::select(-LAT, -LON), by = "grid_df_id") |>
  dplyr::select(word, place_name, analysis_id, shp_id, place, TERRITORY1, country_id) |>  
  distinct(.keep_all = T) |> 
  mutate(country_place = countrycode::countrycode(sourcevar   = place,
                                                  origin      = "country.name",
                                                  destination = "country.name"),
         same_country = country_id == country_place,
         same_territory = TERRITORY1 == country_place)
  # group_by(word, place_name, country_predicted, shp_id, place, TERRITORY1, country_id) |> 
  # summarise(n_articles = n())

uniqueWordPlaceMatchesCountryPred2 <- uniqueWordPlaceMatchesCountryPred |> 
  group_by(word, place_name, shp_id, place, TERRITORY1, country_id) |>
  summarise(n_articles = n())

tmp <- uniqueWordPlaceMatchesCountryPred |> 
  filter(analysis_id == 264937) |>  
  dplyr::select(word, TERRITORY1, country_id) |> 
  distinct()

  nrow(uniqueWordPlaceMatchesCountryPred)
  length(unique(uniqueWordPlaceMatchesCountryPred$analysis_id))

  test_false <- uniqueWordPlaceMatchesCountryPred |> 
    filter((same_country == FALSE & same_territory == FALSE) | (is.na(same_country) & is.na(same_territory))) |> 
    dplyr::select(-same_country, -same_territory) |> 
    group_by(word, place_name, shp_id, place, TERRITORY1, country_id, country_place) |>
    summarise(n_articles = n()) ; nrow(test_false)
  
  tmp <- test_false |> 
    group_by(word, country_id) |> 
    summarise(n = n())
  
  test_true <- uniqueWordPlaceMatchesCountryPred |> 
    filter(same_country == TRUE | same_territory == TRUE) |> 
    dplyr::select(-same_country, -same_territory) ; nrow(test_true)
  
  ### Filter 1 - rows with numbers

    ## Extract all rows with a number
    rows_with_number <- uniqueWordPlaceMatchesCountryPred |> 
      filter(stringr::str_detect(word, ".*[0-9].*"))
  
    ## Filter all rows where there is a number
    uniqueWordPlaceMatchesCountryPred_select1 <- uniqueWordPlaceMatchesCountryPred |> 
      filter(! stringr::str_detect(word, ".*[0-9].*"))
    
  ### Filter 2 - rows matching with the name of a species
    
    ## Filter all rows matching a species name like pattern
    #     ^: Asserts the start of the string.
    # [A-Z]: Matches any uppercase letter from A to Z.
    # \.: Matches a period (dot) character literally.
    # \s: Matches an space character (zero or one space).
    # [a-zA-Z]+: Matches one or more alphabetic characters (both lowercase and uppercase). This ensures that there is at least one letter following the period.
    # (\\s[a-zA-Z]+)?: This is an optional group. It starts with a space (\s) followed by one or more alphabetic characters for the third word. The ? quantifier makes the entire group optional, meaning the third word is optional.
    # $: Asserts the end of the string.
    uniqueWordPlaceMatchesCountryPred_select2 <- uniqueWordPlaceMatchesCountryPred_select1 |> 
      filter(!stringr::str_detect(word, "^[A-Z]\\.\\s[a-zA-Z]+(\\s[a-zA-Z]+)?$")) # "^[A-Z]\\.\\s[a-zA-Z]\\s?[a-z]+$"))
    

  # collect()

nrow(uniqueWordPlaceMatchesCountryPred) # 15553


# Filter to those for which we actually have matches to the natural earth database
# sum(uniqueWordPlaceMatches$place_name %in% )


# uniqueWordPlaceMatches$n_articles <- rep(NA, nrow(uniqueWordPlaceMatches))
# for(i in 1:nrow(uniqueWordPlaceMatches)){
#   uniqueWordPlaceMatches$n_articles[i] <- sum(geoparsedText$word == uniqueWordPlaceMatches$word[i] &
#           geoparsedText$place_name == uniqueWordPlaceMatches$place_name[i])
# }

# Check to see if the tidyverse code gave the same results as the R base code (IT DOES)
# test1 <- geoparsedText %>% 
#   select(word, place_name, analysis_id) %>% 
#   group_by(word, place_name,) |> 
#   summarise(n_articles = n()) |> 
#   collect()
# quantile(test1$n_articles, probs = c(0,0.75, 0.8, 0.85, 0.9, 0.95, 1))



quantile(uniqueWordPlaceMatchesCountryPred$n_articles, probs = c(0,0.75, 0.8, 0.85, 0.9, 0.95, 1))

sum(uniqueWordPlaceMatchesCountryPred$n_articles[17 <= uniqueWordPlaceMatchesCountryPred$n_articles])
sum(uniqueWordPlaceMatchesCountryPred$n_articles[uniqueWordPlaceMatchesCountryPred$n_articles < 17])


highFreqMatches <- uniqueWordPlaceMatchesCountryPred %>%
  filter(17 <= n_articles) %>%
  arrange(desc(n_articles)) 

lowFreqMatches <- uniqueWordPlaceMatchesCountryPred %>%
  filter(n_articles < 17) %>%
  arrange(desc(n_articles)) 

narticle_per_coutry_LowFreq <- lowFreqMatches |> 
  group_by(country_predicted) |>
  summarise(n_articles = n()) |>
  arrange(desc(n_articles)) 

nrow(highFreqMatches) # 437 articles to check

write.csv(highFreqMatches, file = here::here("data/geoparsing/geoparsing_wordPlaceMatches95quantile.csv"))


shp_df <- tbl(dbcon, "shp_df_natural-earth-shapes") %>%
  select(name,name_long, name_en,name_local, abbrev, formal_en,adm0_a3_us, continent,region_un,region, subregion,region_wb,gn_name, shpfile_id) %>%
  collect()

shp_df_names <- apply(shp_df, 1, function(x) paste(x, collapse = " "))

shp_df[grep("Everglade", shp_df_names, ignore.case = TRUE),] %>% View
shp_df[grep("africa", shp_df_names, ignore.case = TRUE),c("name","shpfile_id")] %>% View

temp <- shp_df[grep("eastern africa", shp_df$subregion, ignore.case = TRUE),]
temp <- temp %>% filter(subregion %in% c("Melanesia","Micronesia","Polynesia"))
paste(temp$name, collapse = "; ")
paste(temp$shpfile_id, collapse = "; ")

shp_df_matches[grep("everglade", shp_df_matches$place, ignore.case = TRUE),] %>%
  filter(!is.na(shp_id)) %>%
  distinct(place, shp_id) %>%
  select(place, shp_id) %>%
  View()


# After cleaning, read in data and visualize
highFreqMatches_clean <- read.csv(here::here("data/geoparsing/geoparsing_wordPlaceMatches95quantile_cleaned.csv"))

highFreqMatches_clean %>%
  group_by(true_match, ambiguous) %>%
  summarise(n_articles = sum(n_articles))

sum(highFreqMatches_clean$shp_id_correct!= "")
sum(highFreqMatches_clean$true_match == FALSE)
sum(highFreqMatches_clean$n_articles[highFreqMatches_clean$shp_id_correct!= ""])

highFreqMatches_clean %>%
  group_by(true_match, ambiguous) %>%
  summarise(n_articles = sum(n_articles)) %>%
  reshape2::melt(value.var = "n_articles") %>%
  ggplot(aes(x=true_match, y=value, fill = ambiguous))+
  geom_col(position = "stack")+
  theme_bw()

```


```{r estimate sample size needed to detect error}

## Step 1: Power analysis
# This analysis can be used to estimate what sample size (ie the number of records that need to be checked) in order to estimate the proportion of error for each country

n_mismatch <- pwr::pwr.p.test(h = 0.10, # estimate
           sig.level = 0.05,
           power = 0.80,
           alternative = "greater")

# The total number of rows needed to be screened in order to quantify/detect error needs to be multiplied by number of countries
formatC(ceiling(n_mismatch$n)*length(unique(geoparsedText$country_predicted)), big.mark = ",", format="d")
# 139,275

# This number is  >> than the total number of rows, so will need to screen/check all the rows in the data

```

```{r Add fixes to new sqlite database v5}
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, "all_tables_v5.sqlite"), create=FALSE)

# Write over old tables
dbWriteTable(conn=dbcon, name="geoparsed_text", value=geoparsedText, append=FALSE, overwrite = TRUE)

dbWriteTable(conn=dbcon, name="geoparsed-text_shp_df_matches", value=shp_df_matches2, append=FALSE, overwrite = TRUE)

dbWriteTable(conn=dbcon, name="geoparsed-text_grid-sums", value=shp_df_sum, append=FALSE, overwrite = TRUE)


DBI::dbDisconnect(dbcon)
```




```{r disconnect from database}
DBI::dbDisconnect(dbcon)
```

# Combine ORO_type predictions into long format

the predictions for the different ORO types are grouped into separate tables based on which branch they are relevant for. So make sure to combine them all, and order them along the x axis so that they are grouped by ORO branch (i.e. Mitigation vs Natural vs Societal) and not alphabetically. To faciliate this, create a new data table and add it to the sqlite database

```{r oro_type_long format}
# Connect to latest version of the database
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, latestVersion), create=FALSE)


# Format the dataframes so they're in long format, 
# with ID columns for analysis_id and oro_branch and oro_type, prediction boundary, 
# and value column for prediction
branchNames <- c("mitigation","nature","societal")
branchNamesFull <- c("Mitigation","Natural resilience", "Societal adaptation")
predOROAnyList <- list() # empty list to store results

for(i in 1:length(branchNames)){
  tempTbl <- tbl(dbcon, paste("pred_oro_any", branchNames[i], sep = "_")) %>%
    collect()
  # only look at columns with analysis id and mean predictions
  #meanCols <- colnames(tempTbl)[grep("mean", colnames(tempTbl))]
  #tempTbl <- tempTbl[,c("analysis_id", meanCols)]
  
  # rename columns 
  # colNames <- gsub("\\ -.*","",meanCols)
  # colNames <- gsub("oro_any.","", colNames)
  # colNames <- gsub("M_","", colNames)
  # colNames <- gsub("N_","", colNames)
  # colNames <- gsub("SA_","", colNames)
  # colnames(tempTbl)[which(colnames(tempTbl) %in% meanCols)] <- colNames
  colNames <- colnames(tempTbl)
  colNames[1] <- "analysis_id"
  colNames <- gsub("oro_any.","",colNames)
  colNames <- gsub("M_","", colNames)
  colNames <- gsub("N_","", colNames)
  colNames <- gsub("SA_","", colNames)
  colNames <- gsub(" - ","_", colNames)
  colNames <- gsub("_prediction","", colNames)
  colNames <- gsub("_pred","", colNames)
  colnames(tempTbl) <- colNames
  
  # Melt and save data frame
  tempTbl$oro_branch = paste(branchNamesFull[i])
  predOROAnyList[[i]] <- tempTbl
  #tempTbl <- reshape2::melt(tempTbl, variable.name = "oro_type", value.name = "mean_prediction", 
  #                         id.vars = c("analysis_id","oro_branch"))
  # if(b == 1){
  #   oro_type_long <- tempTbl
  # }else{
  #   oro_type_long <- rbind(oro_type_long, tempTbl)
  # }
}


## Make a dataframe of the predictions
for(i in 1:length(predOROAnyList)){
  temp <- predOROAnyList[[i]]
  boundaries <- c("upper","mean","lower")
  for(b in 1:length(boundaries)){
    temp_b <- cbind(temp[,1], temp$oro_branch, temp[grep(boundaries[b], colnames(temp))])
    colnames(temp_b)[2]<- "oro_branch"
    colnames(temp_b) <- gsub(paste0("_",boundaries[b]),"", colnames(temp_b))
    temp_b <- reshape2::melt(temp_b, variable.name = "oro_type", value.name = boundaries[b], 
                             id.vars = c("analysis_id","oro_branch"))
    if(b==1){
      temp_melt <- temp_b
    }else{
      temp_melt <- merge(temp_melt, temp_b, by=c("analysis_id","oro_branch","oro_type"))
    }
  }
  if(i==1)
    pred_oro_type_long <- temp_melt
  else
    pred_oro_type_long <- rbind(pred_oro_type_long, temp_melt)
}
rm(predOROAnyList)



# Format the dataframe by factoring variables
pred_oro_type_long <- pred_oro_type_long %>%
  filter(!(oro_type %in% c("Protection", "Restoration"))) %>%
  mutate(
    oro_branch = factor(oro_branch, levels = branchNamesFull),
    oro_type = factor(oro_type,
                      levels = c("Renewables","Increase_efficiency", "CO2_removal_or_storage",
                                 "Human_assisted_evolution","Conservation",
                                 "Built_infrastructure_and_technology","Socioinstitutional"),
                      labels = c("Marine renewable energy","Increase efficiency", "CO2 removal or storage",
                                 "Human assisted evolution","Conservation",
                                 "Built infrastructure & technology","Socio-institutional"))
  )


## Write in to sqlite database
dbWriteTable(conn=dbcon, name="pred_oro_type_long", value=pred_oro_type_long, append=FALSE, overwrite = FALSE)


DBI::dbDisconnect(dbcon)
```


# Write test list after coding (n=86)

```{r write test list}
# From analysis file: 08
# data load
# get screen results after coding
# Connect to latest version of the database
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, latestVersion), create=FALSE)

seen_screen <- tbl(dbcon, "seen_screen")
dedups <- tbl(dbcon, "uniquerefs")

test_list_final <- seen_screen %>%
  filter(sample_screen == "test list"& include_screen == 1) %>%
  left_join(dedups %>% select(analysis_id, year, author, doi), by= "analysis_id") %>%
  select(analysis_id, title, abstract, keywords, year, author, doi) %>%
  collect()

nrow(test_list_final) # 86

dbDisconnect(dbcon)



writexl::write_xlsx(test_list_final, here::here("outputs/test_list.xlsx"))


```



