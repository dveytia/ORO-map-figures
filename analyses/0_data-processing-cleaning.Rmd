---
title: "0_data-processing-cleaning"
author: "Devi Veytia"
date: "2023-11-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries}
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)
library(countrycode)
```

# Add files to sqlite database

```{r Get the latest version of sqlite database}

sqliteDir <- here::here("data/sqlite-databases")
sqliteFiles <- dir(sqliteDir)
sqliteVersions <- as.numeric(gsub(".sqlite","",substring(sqliteFiles, regexpr("_v", sqliteFiles) + 2)))
latestVersion <- sqliteFiles[which.max(sqliteVersions)]


```



## Add geoparsing files

```{r add geoparsing results for all the geoparsed text}

# Connect to latest version of the database
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, latestVersion), create=FALSE)

# Get info about geoparsed text
geoparsedTextDir <- here::here("data/geoparsing/geoparsed-text")
geoparsedTextFiles <- dir(geoparsedTextDir)
tblName <- "geoparsed_text"

# Loop through csv files and write to database
for(i in 1:length(geoparsedTextFiles)){
  # Read in the csv file
  df <- readr::read_csv(file = file.path(geoparsedTextDir, geoparsedTextFiles[i]), show_col_types = FALSE)
  df <- df %>%
    rename(analysis_id = id)
  # If starting the table, don't need to append, if already there, then yes append
  if(i == 1){
    appendAction = FALSE
  }else{
    appendAction = TRUE
  }
  # Write to database
  dbWriteTable(conn=dbcon,name=tblName, value=df, append=appendAction, overwrite = FALSE)
}

DBI::dbDisconnect(dbcon) # DIsconnect
```

```{r add other geoparsing tables}
# Connect to latest version of the database
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, latestVersion), create=FALSE)

# Get a list of all the geoparsing file names
geoparsingDir <- here::here("data/geoparsing")
geoparsingFiles <- dir(geoparsingDir)
geoparsingFiles <- geoparsingFiles[grep(".csv", geoparsingFiles)]
geoparsingTableNames <- gsub(".csv","", geoparsingFiles)
geoparsingTableNames[-grep("grid_df_res", geoparsingTableNames)] <- gsub('[0-9]+', '', geoparsingTableNames[-grep("grid_df_res", geoparsingTableNames)])
geoparsingTableNames <- unique(geoparsingTableNames)

## Loop through all the tables and write to the database
for(i in 1:length(geoparsingTableNames)){
  # identify latest version of the file
  files <- geoparsingFiles[grep(geoparsingTableNames[i], geoparsingFiles)]
  fileVersions <- substr(files, nchar(geoparsingTableNames[i])+1, nchar(geoparsingTableNames[i])+1)
  if("." %in% fileVersions){
    fileVersions[which(fileVersions == ".")] <- 0
  }
  fileVersions <- as.numeric(fileVersions)
  file <- files[which.max(fileVersions)]
  
  # Read in file
  df <- readr::read_csv(file = file.path(geoparsingDir, file), show_col_types = FALSE)
  # Have to add this because some column names are duplicates (one is upper, the other is lower case)
  if(geoparsingTableNames[i] == "shp_df_natural-earth-shapes"){
    colnames(df) <- make.unique(tolower(colnames(df)), sep = "_")
  }
  # Write to database
  dbWriteTable(conn=dbcon, name=geoparsingTableNames[i], value=df, append=FALSE, overwrite = TRUE)
}

src_dbi(dbcon)

DBI::dbDisconnect(dbcon)
```


```{r additional data cleaning of false associations between country matches for v3}
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, "all_tables_v3.sqlite"), create=FALSE)

example_ids <- c(35143, 527)

geoparsed_text <- tbl(dbcon, "geoparsed_text") %>%
  filter(analysis_id %in% example_ids) %>%
  collect()

shp_df_matches <- tbl(dbcon, "geoparsed-text_shp_df_matches") %>%
  filter(analysis_id %in% example_ids) %>%
  collect()

grid_df <- tbl(dbcon, "grid_df_res2.5") %>%
  collect() 

grid_df2 <- grid_df %>% right_join(shp_df_matches, by = "grid_df_id")


## quick plot
plot(grid_df$LON, grid_df$LAT)

# So the geoparsing and matching to shapefiles looks fine, but its the intersection between shpfile polygons to grid ids that looks to be the problem. Try in R? The relevant shp_id is 31



##  Recalculate shp_id to grid_df_id matching in R -----------
shp_grid_df <- read.csv("C:\\Users\\deviv\\R-working-folder\\spacy_example\\outputs\\shp_grid_df.csv")

countries <- map_data("world")

shp_grid_df %>%
  filter(shpfile_id == 31) %>%
  left_join(grid_df, by = "grid_df_id") %>%
  View() 

shp_grid_df %>%
  filter(shpfile_id == 31) %>%
  left_join(grid_df, by = "grid_df_id") %>%
  ggplot(aes(LON, LAT))+
  geom_point(col="red")+
  geom_polygon(data = countries, aes(long, lat, group=group))

# The version calculated in my spacy script seems good, so use this and make the sqlite file version 2

shp_df_matches <- tbl(dbcon, "geoparsed-text_shp_df_matches") %>% collect()

# Separate where there is and isn't a shp_id
shp_df_matches_noID <- shp_df_matches %>% filter(is.na(shp_id))
shp_df_matches_ID <- shp_df_matches %>% filter(!is.na(shp_id))

# Get unique combinations of analysis id, shp id, and place to rematch to grid cells
shp_df_matches_ID_unique <- shp_df_matches_ID[c("analysis_id","shp_id","place")]
shp_df_matches_ID_unique <- shp_df_matches_ID_unique[!duplicated(shp_df_matches_ID_unique),]

# for each shp_id, add new rows for all the relevant grid cells
shp_df_matches_ID_unique <- merge(shp_df_matches_ID_unique, shp_grid_df %>% rename(shp_id = "shpfile_id"),
                                  by = "shp_id", all.x = TRUE, all.y = FALSE)


# Double check again shp_id 31 (the UK) -- yes
shp_df_matches_ID_unique %>%
  filter(shp_id == 31) %>%
  left_join(grid_df, by = "grid_df_id") %>%
  ggplot(aes(LON, LAT))+
  geom_point(col="red")+
  geom_polygon(data = countries, aes(long, lat, group=group))



# Calculate the grid cell weight
## For each unique document and place, calculate the 1/the number of grid cells occupied by that place to get the grid cell weight
shp_df_matches_ID_unique <- shp_df_matches_ID_unique %>%
  group_by(analysis_id, shp_id) %>%
  mutate(cell_weight = 1/n())

# noticed that where there was no shp_id, cell weight was NA so recalculate grouping by place
shp_df_matches_noID <- shp_df_matches_noID %>%
  group_by(analysis_id, place) %>%
  mutate(cell_weight = 1/n())

## Rejoin new shp_id to grid matches with those rows that didn't have a shp_id
shp_df_matches2 <- rbind(shp_df_matches_ID_unique, shp_df_matches_noID)


## Recalculate grid sums
shp_df_sum = shp_df_matches2 %>%
  group_by(grid_df_id) %>%
  summarise(n_articles = n(), n_articles_weighted = sum(cell_weight, na.rm=TRUE)) %>%
  right_join(grid_df, by = "grid_df_id")

# quick plot
shp_df_sum %>%
  ggplot()+
  geom_tile(aes(LON, LAT, fill = log(n_articles_weighted)))+
  scale_fill_viridis_c(option = "D")+
  geom_polygon(data = countries, aes(long, lat, group=group), fill="transparent")
  #coord_map(projection = "mercator")


DBI::dbDisconnect(dbcon)

```

```{r Add fixes to new sqlite database v4}
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, "all_tables_v4.sqlite"), create=FALSE)

# Write over old tables
dbWriteTable(conn=dbcon, name="geoparsed-text_shp_df_matches", value=shp_df_matches2, append=FALSE, overwrite = TRUE)

dbWriteTable(conn=dbcon, name="shp_grid_df", value=shp_grid_df, append=FALSE, overwrite = TRUE)

dbWriteTable(conn=dbcon, name="geoparsed-text_grid-sums", value=shp_df_sum, append=FALSE, overwrite = TRUE)


DBI::dbDisconnect(dbcon)
```


## Clean name matching errors - v5

This section will clean the bad place matches from the geoparsing tables.

Steps to clean:
0. Select only empirical papers
1. Select only data were a the place name matches a shapefile ID
2. Remove acronym matches where the word matched is all capitalized letters, except UK and US
3. Remove text between copyright symbol and next period (get rid of publisher matching)
4. Remove land matches x distance from the coast when the spatial scale of the match is sub-national
5. Remove affiliation text from abstract, and any resulting word matches
6. Remove word matches that are a complete word in the English language
7. Weight match by country_conf value (0.6 - 1)** -> consider removing low values using cluster analysis altogether


Notes on no. 4: 
There are several definitions we can use to identify coastal land (from least to most conservative):
a. Within 100 km from the land-sea interface (WRI 2000,Burke et al. 2001)
b. Landward area contained within 100 m elevation of sea level (Nicholls and Small 2002)
c. Landward area contained within the 200 m land elevation contour ((Land-ocean interaction in the coastal zone programme -- now Future Earth Coasts)[https://www.researchgate.net/publication/226867603_The_Coastal_Zone_-_a_Domain_of_Global_Interactions], Pernettaand Milliman 1995). This is designed to capture the area "extending from the coastal plains to the outer edge of the continental shelves, approximately matching the region that has been alternatively flooded and exposed during the sea level fluctuations of the late Quaternary period”(Holligan and de Boois 1993). 

I suggest to take the conservative approach of option "c" and do landward area within 200 m land elevation contour. That said, if it is too complicated/time consuming, then option "a" looks easier -- we could make the argument that we extend it to 200 km from coast because given sea level rise locations further inland may be concerned with coastal research? 

*Estimating error rate*

Another approach is to create a lookup table of unique word and place_name matches and double check manually -- this can then be merged into the geoparsed_text table to indicate whether a match is TRUE, FALSE, or ambiguous. This will allow us to estimate the likely error rate, which can be stratified by country. This information can be used to:
a) clean the data by completely eliminating matches that are FALSE
b) estimate "uncertainty" for each country using the "ambiguous" classification -- perhaps this information can be displayed alongside the data plotted in the map via stippling etc. This will likely apply to countries like the USA, where records like "south beach" are attributed.



```{r load data}
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, "all_tables_v4.sqlite"), create=FALSE)

geoparsedText <- tbl(dbcon, "geoparsed_text") 
shp_df_matches <- tbl(dbcon, "geoparsed-text_shp_df_matches") 
shp_df_sum <- tbl(dbcon, "geoparsed-text_grid-sums")
uniquerefs <- tbl(dbcon, "uniquerefs")
shp_df_matches <- tbl(dbcon, "geoparsed-text_shp_df_matches") 
grid_df <- tbl(dbcon, "grid_df_res2.5") %>% collect() 
```


```{r steps for cleaning}

### Global geoparsed data
geoP_data <- tbl(dbcon, "geoparsed_text") |>  
    dplyr::select(analysis_id, word, country_predicted, place_name) |> 
    collect() 

length(unique(geoP_data$analysis_id))
nrow(geoP_data)

### Note N°0: Select only empirical studies

  ## Extract the papers predicted as empirical ones
  predMethod <- tbl(dbcon, "pred_method_type") |> 
    select(analysis_id, 
           `method_type.Empirical - mean_prediction`) |> 
    filter(`method_type.Empirical - mean_prediction` >= 0.5) |> 
    collect()
  
  ## Selection of empiral papers among all papers
  geoP_data_empirical <- filter(geoP_data, analysis_id %in% unique(predMethod$analysis_id))

  length(unique(geoP_data_empirical$analysis_id))
  nrow(geoP_data_empirical)

  
### Note N°1: Select only data where !is.na()
  
  ## Join with the geoparsed matching location (grid cell level)
  geoP_data_empirical_cell <- geoP_data_empirical |> 
    left_join(shp_df_matches, by = "analysis_id", copy = TRUE) |>
    collect()
  
  nrow(geoP_data_empirical_cell)
  length(unique(geoP_data_empirical_cell$analysis_id))
  
  ## Filter only rows where there the shp_id column != NA
  geoP_data_empirical_cell_noNA <- filter(geoP_data_empirical_cell, !is.na(shp_id))
  nrow(geoP_data_empirical_cell_noNA)
  length(unique(geoP_data_empirical_cell_noNA$analysis_id))
  
  
### Note N°2: Capital letter filtering
  
  # (Filter row that are not in capital letter) OR
  # (Filter row in capital letters & with a number of character > 3) OR ~ to avoid acronyme but keep city name in capital letters
  # (Filter row in capital letters & matching with USA, UK or US) ~ to avoid loosing data on these 2 countries
  geoP_data_empirical_cell_acronym <- geoP_data_empirical_cell |> 
    filter((stringr::str_detect(word, pattern = "^[[:upper:][:space:]]+$") == FALSE) |
           (stringr::str_detect(word, pattern = "^[[:upper:][:space:]]+$") == TRUE & nchar(word) > 3) |
           (stringr::str_detect(word, pattern = "^[[:upper:][:space:]]+$") == TRUE & word %in% c("USA", "UK", "US")))
  
  nrow(geoP_data_empirical_cell_acronym)
  length(unique(geoP_data_empirical_cell_acronym$analysis_id))

  
### Note N°3: Remove word matching with copyright informations
  
  ## Extract analysis_id that have (c) symbole in the abstract
  geoparsedText_with_C <- uniquerefs |>
    collect() |> 
    # Filter only analysis_id keept at previous filter
    filter(analysis_id %in% unique(geoP_data_empirical_cell_acronym$analysis_id)) |> 
    # Filter only analysis_id with the copyright symbole
    filter(stringr::str_detect(abstract, pattern = "Â©")) |> 
    # mutate(copyright_text = stringr::str_extract(abstract, "\\Â©\\w+$"))
    mutate(copyright_text = stringr::str_split(abstract, "Â©")[[1]][2]) |> 
    select(analysis_id, copyright_text)
  
  
  ## Filter data with "word" matching with a name in "copyright_text
  geoP_data_empirical_cell_acronym_CR <- geoP_data_empirical_cell_acronym |> 
    left_join(geoparsedText_with_C, by = "analysis_id") |> 
    mutate(filter = case_when(stringr::str_detect(copyright_text, stringr::fixed(word)) ~ TRUE,
                              TRUE ~ FALSE)) |> 
    filter(filter == FALSE) |> 
    select(-copyright_text, -filter)
  
  nrow(geoP_data_empirical_cell_acronym_CR)
  length(unique(geoP_data_empirical_cell_acronym_CR$analysis_id))
  
  
### Note N°4: Remove land matches x distance from the coast when the spatial scale of the match is sub-national
  
  ## Set new grid dataframe with new info
  grid_df2 <- grid_df
  
  ## Within the EEZ----------------------------
  require(raster)
  eez_rast <- raster::raster(here::here("data", "external", "eez_rast", "rast_iso_eez.asc"))
  eez_rast[eez_rast == 0] <- NA
  
    # extract eez value at grid cell points
    grid_df2$eez_vals <- raster::extract(eez_rast, grid_df[,c("LON","LAT")])
    
  ## Distiance inlad from the coast 
    
    # Used this as inspo: https://dominicroye.github.io/en/2019/calculating-the-distance-to-the-sea-in-r/
    coast <- giscoR::gisco_get_countries(
      spatialtype = "COASTL",
      epsg = "4326"
    )
    
    coast_utm <- sf::st_transform(coast, 4087)
  
    landCells_sf <- grid_df |>  
      dplyr::filter(is_land == 1) |> 
      sf::st_as_sf(coords = c('LON','LAT')) |> 
      sf::st_set_crs(4326) |> 
      sf::st_transform(4087)

    #transform from polygon shape to line
    coast_utm <- sf::st_cast(coast_utm, "MULTILINESTRING")
    
    #calculation of the distance between the coast and our points
    dist <- sf::st_distance(coast_utm, landCells_sf)
    
    # will need to compute the minimum distance for each land cell
    minDist <- apply(dist, 2, function(x) min(x, na.rm=T))
    minDist <- minDist/1000 # convert to km
    landCells_sf$distInLand_km <- minDist
    landCells <- as.data.frame(landCells_sf)
    
    grid_df2 <- grid_df2 |> 
      dplyr::left_join(landCells |>  dplyr::select(grid_df_id, distInLand_km))
    
    # Plot to check -- this works!
    ggplot(grid_df2) +
      geom_point(aes(LON, LAT, col = distInLand_km))
    

  ## Make a new column to identify inland or coast
  grid_df2 <- grid_df2 %>%
    mutate(coastal = ifelse(distInLand_km <= 200 | !is.na(eez_vals), 1, 0))
  
    # check
    grid_df2 %>%
      ggplot(aes(LON, LAT, fill=coastal))+
      geom_tile()
    
  ## Filter only cells that are at least 200km from coast
  geoP_data_empirical_cell_acronym_CR_200km <- geoP_data_empirical_cell_acronym_CR |> 
    left_join(grid_df2 |> select(grid_df_id, coastal), by = "grid_df_id") |> 
    filter(coastal == 1)
    
  nrow(geoP_data_empirical_cell_acronym_CR_200km)
  length(unique(geoP_data_empirical_cell_acronym_CR_200km$analysis_id))

```

```{r visualize data structure}
# uniqueWordPlaceMatchesCountryPred <- geoparsedText %>% select(word, place_name) %>% distinct(word, place_name) 

uniqueWordPlaceMatchesCountryPred <- geoparsedText %>% 
  select(word, place_name, analysis_id, country_predicted) %>% 
  group_by(word, place_name, country_predicted) |> 
  summarise(n_articles = n()) |> 
  collect()

nrow(uniqueWordPlaceMatchesCountryPred) # 8261



# Filter to those for which we actually have matches to the natural earth database
# sum(uniqueWordPlaceMatches$place_name %in% )


# uniqueWordPlaceMatches$n_articles <- rep(NA, nrow(uniqueWordPlaceMatches))
# for(i in 1:nrow(uniqueWordPlaceMatches)){
#   uniqueWordPlaceMatches$n_articles[i] <- sum(geoparsedText$word == uniqueWordPlaceMatches$word[i] &
#           geoparsedText$place_name == uniqueWordPlaceMatches$place_name[i])
# }

# Check to see if the tidyverse code gave the same results as the R base code (IT DOES)
# test1 <- geoparsedText %>% 
#   select(word, place_name, analysis_id) %>% 
#   group_by(word, place_name,) |> 
#   summarise(n_articles = n()) |> 
#   collect()
# quantile(test1$n_articles, probs = c(0,0.75, 0.8, 0.85, 0.9, 0.95, 1))



quantile(uniqueWordPlaceMatchesCountryPred$n_articles, probs = c(0,0.75, 0.8, 0.85, 0.9, 0.95, 1))

sum(uniqueWordPlaceMatchesCountryPred$n_articles[17 <= uniqueWordPlaceMatchesCountryPred$n_articles])
sum(uniqueWordPlaceMatchesCountryPred$n_articles[uniqueWordPlaceMatchesCountryPred$n_articles < 17])


highFreqMatches <- uniqueWordPlaceMatchesCountryPred %>%
  filter(17 <= n_articles) %>%
  arrange(desc(n_articles)) 

lowFreqMatches <- uniqueWordPlaceMatchesCountryPred %>%
  filter(n_articles < 17) %>%
  arrange(desc(n_articles)) 

narticle_per_coutry_LowFreq <- lowFreqMatches |> 
  group_by(country_predicted) |>
  summarise(n_articles = n()) |>
  arrange(desc(n_articles)) 

nrow(highFreqMatches) # 437 articles to check

write.csv(highFreqMatches, file = here::here("data/geoparsing/geoparsing_wordPlaceMatches95quantile.csv"))


shp_df <- tbl(dbcon, "shp_df_natural-earth-shapes") %>%
  select(name,name_long, name_en,name_local, abbrev, formal_en,adm0_a3_us, continent,region_un,region, subregion,region_wb,gn_name, shpfile_id) %>%
  collect()

shp_df_names <- apply(shp_df, 1, function(x) paste(x, collapse = " "))

shp_df[grep("Everglade", shp_df_names, ignore.case = TRUE),] %>% View
shp_df[grep("africa", shp_df_names, ignore.case = TRUE),c("name","shpfile_id")] %>% View

temp <- shp_df[grep("eastern africa", shp_df$subregion, ignore.case = TRUE),]
temp <- temp %>% filter(subregion %in% c("Melanesia","Micronesia","Polynesia"))
paste(temp$name, collapse = "; ")
paste(temp$shpfile_id, collapse = "; ")

shp_df_matches[grep("everglade", shp_df_matches$place, ignore.case = TRUE),] %>%
  filter(!is.na(shp_id)) %>%
  distinct(place, shp_id) %>%
  select(place, shp_id) %>%
  View()


# After cleaning, read in data and visualize
highFreqMatches_clean <- read.csv(here::here("data/geoparsing/geoparsing_wordPlaceMatches95quantile_cleaned.csv"))

highFreqMatches_clean %>%
  group_by(true_match, ambiguous) %>%
  summarise(n_articles = sum(n_articles))

sum(highFreqMatches_clean$shp_id_correct!= "")
sum(highFreqMatches_clean$true_match == FALSE)
sum(highFreqMatches_clean$n_articles[highFreqMatches_clean$shp_id_correct!= ""])

highFreqMatches_clean %>%
  group_by(true_match, ambiguous) %>%
  summarise(n_articles = sum(n_articles)) %>%
  reshape2::melt(value.var = "n_articles") %>%
  ggplot(aes(x=true_match, y=value, fill = ambiguous))+
  geom_col(position = "stack")+
  theme_bw()

```


```{r estimate sample size needed to detect error}

## Step 1: Power analysis
# This analysis can be used to estimate what sample size (ie the number of records that need to be checked) in order to estimate the proportion of error for each country

n_mismatch <- pwr::pwr.p.test(h = 0.10, # estimate
           sig.level = 0.05,
           power = 0.80,
           alternative = "greater")

# The total number of rows needed to be screened in order to quantify/detect error needs to be multiplied by number of countries
formatC(ceiling(n_mismatch$n)*length(unique(geoparsedText$country_predicted)), big.mark = ",", format="d")
# 139,275

# This number is  >> than the total number of rows, so will need to screen/check all the rows in the data

```

```{r Add fixes to new sqlite database v5}
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, "all_tables_v5.sqlite"), create=FALSE)

# Write over old tables
dbWriteTable(conn=dbcon, name="geoparsed_text", value=geoparsedText, append=FALSE, overwrite = TRUE)

dbWriteTable(conn=dbcon, name="geoparsed-text_shp_df_matches", value=shp_df_matches2, append=FALSE, overwrite = TRUE)

dbWriteTable(conn=dbcon, name="geoparsed-text_grid-sums", value=shp_df_sum, append=FALSE, overwrite = TRUE)


DBI::dbDisconnect(dbcon)
```




```{r disconnect from database}
DBI::dbDisconnect(dbcon)
```

# Combine ORO_type predictions into long format

the predictions for the different ORO types are grouped into separate tables based on which branch they are relevant for. So make sure to combine them all, and order them along the x axis so that they are grouped by ORO branch (i.e. Mitigation vs Natural vs Societal) and not alphabetically. To faciliate this, create a new data table and add it to the sqlite database

```{r oro_type_long format}
# Connect to latest version of the database
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, latestVersion), create=FALSE)


# Format the dataframes so they're in long format, 
# with ID columns for analysis_id and oro_branch and oro_type, prediction boundary, 
# and value column for prediction
branchNames <- c("mitigation","nature","societal")
branchNamesFull <- c("Mitigation","Natural resilience", "Societal adaptation")
predOROAnyList <- list() # empty list to store results

for(i in 1:length(branchNames)){
  tempTbl <- tbl(dbcon, paste("pred_oro_any", branchNames[i], sep = "_")) %>%
    collect()
  # only look at columns with analysis id and mean predictions
  #meanCols <- colnames(tempTbl)[grep("mean", colnames(tempTbl))]
  #tempTbl <- tempTbl[,c("analysis_id", meanCols)]
  
  # rename columns 
  # colNames <- gsub("\\ -.*","",meanCols)
  # colNames <- gsub("oro_any.","", colNames)
  # colNames <- gsub("M_","", colNames)
  # colNames <- gsub("N_","", colNames)
  # colNames <- gsub("SA_","", colNames)
  # colnames(tempTbl)[which(colnames(tempTbl) %in% meanCols)] <- colNames
  colNames <- colnames(tempTbl)
  colNames[1] <- "analysis_id"
  colNames <- gsub("oro_any.","",colNames)
  colNames <- gsub("M_","", colNames)
  colNames <- gsub("N_","", colNames)
  colNames <- gsub("SA_","", colNames)
  colNames <- gsub(" - ","_", colNames)
  colNames <- gsub("_prediction","", colNames)
  colNames <- gsub("_pred","", colNames)
  colnames(tempTbl) <- colNames
  
  # Melt and save data frame
  tempTbl$oro_branch = paste(branchNamesFull[i])
  predOROAnyList[[i]] <- tempTbl
  #tempTbl <- reshape2::melt(tempTbl, variable.name = "oro_type", value.name = "mean_prediction", 
  #                         id.vars = c("analysis_id","oro_branch"))
  # if(b == 1){
  #   oro_type_long <- tempTbl
  # }else{
  #   oro_type_long <- rbind(oro_type_long, tempTbl)
  # }
}


## Make a dataframe of the predictions
for(i in 1:length(predOROAnyList)){
  temp <- predOROAnyList[[i]]
  boundaries <- c("upper","mean","lower")
  for(b in 1:length(boundaries)){
    temp_b <- cbind(temp[,1], temp$oro_branch, temp[grep(boundaries[b], colnames(temp))])
    colnames(temp_b)[2]<- "oro_branch"
    colnames(temp_b) <- gsub(paste0("_",boundaries[b]),"", colnames(temp_b))
    temp_b <- reshape2::melt(temp_b, variable.name = "oro_type", value.name = boundaries[b], 
                             id.vars = c("analysis_id","oro_branch"))
    if(b==1){
      temp_melt <- temp_b
    }else{
      temp_melt <- merge(temp_melt, temp_b, by=c("analysis_id","oro_branch","oro_type"))
    }
  }
  if(i==1)
    pred_oro_type_long <- temp_melt
  else
    pred_oro_type_long <- rbind(pred_oro_type_long, temp_melt)
}
rm(predOROAnyList)



# Format the dataframe by factoring variables
pred_oro_type_long <- pred_oro_type_long %>%
  filter(!(oro_type %in% c("Protection", "Restoration"))) %>%
  mutate(
    oro_branch = factor(oro_branch, levels = branchNamesFull),
    oro_type = factor(oro_type,
                      levels = c("Renewables","Increase_efficiency", "CO2_removal_or_storage",
                                 "Human_assisted_evolution","Conservation",
                                 "Built_infrastructure_and_technology","Socioinstitutional"),
                      labels = c("Marine renewable energy","Increase efficiency", "CO2 removal or storage",
                                 "Human assisted evolution","Conservation",
                                 "Built infrastructure & technology","Socio-institutional"))
  )


## Write in to sqlite database
dbWriteTable(conn=dbcon, name="pred_oro_type_long", value=pred_oro_type_long, append=FALSE, overwrite = FALSE)


DBI::dbDisconnect(dbcon)
```


# Write test list after coding (n=86)

```{r write test list}
# From analysis file: 08
# data load
# get screen results after coding
# Connect to latest version of the database
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, latestVersion), create=FALSE)

seen_screen <- tbl(dbcon, "seen_screen")
dedups <- tbl(dbcon, "uniquerefs")

test_list_final <- seen_screen %>%
  filter(sample_screen == "test list"& include_screen == 1) %>%
  left_join(dedups %>% select(analysis_id, year, author, doi), by= "analysis_id") %>%
  select(analysis_id, title, abstract, keywords, year, author, doi) %>%
  collect()

nrow(test_list_final) # 86

dbDisconnect(dbcon)



writexl::write_xlsx(test_list_final, here::here("outputs/test_list.xlsx"))


```



