---
title: "0_data-processing-cleaning"
author: "Devi Veytia"
date: "2023-11-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries}
library(dplyr)
library(dbplyr)
library(R.utils)
library(RSQLite)

```

# Add files to sqlite database

```{r Get the latest version of sqlite database}

sqliteDir <- here::here("data/sqlite-databases")
sqliteFiles <- dir(sqliteDir)
sqliteVersions <- as.numeric(gsub(".sqlite","",substring(sqliteFiles, regexpr("_v", sqliteFiles) + 2)))
latestVersion <- sqliteFiles[which.max(sqliteVersions)]


```



## Add geoparsing files

```{r add geoparsing results for all the geoparsed text}

# Connect to latest version of the database
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, latestVersion), create=FALSE)

# Get info about geoparsed text
geoparsedTextDir <- here::here("data/geoparsing/geoparsed-text")
geoparsedTextFiles <- dir(geoparsedTextDir)
tblName <- "geoparsed_text"

# Loop through csv files and write to database
for(i in 1:length(geoparsedTextFiles)){
  # Read in the csv file
  df <- readr::read_csv(file = file.path(geoparsedTextDir, geoparsedTextFiles[i]), show_col_types = FALSE)
  df <- df %>%
    rename(analysis_id = id)
  # If starting the table, don't need to append, if already there, then yes append
  if(i == 1){
    appendAction = FALSE
  }else{
    appendAction = TRUE
  }
  # Write to database
  dbWriteTable(conn=dbcon,name=tblName, value=df, append=appendAction, overwrite = FALSE)
}

DBI::dbDisconnect(dbcon) # DIsconnect
```

```{r add other geoparsing tables}
# Connect to latest version of the database
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, latestVersion), create=FALSE)

# Get a list of all the geoparsing file names
geoparsingDir <- here::here("data/geoparsing")
geoparsingFiles <- dir(geoparsingDir)
geoparsingFiles <- geoparsingFiles[grep(".csv", geoparsingFiles)]
geoparsingTableNames <- gsub(".csv","", geoparsingFiles)
geoparsingTableNames[-grep("grid_df_res", geoparsingTableNames)] <- gsub('[0-9]+', '', geoparsingTableNames[-grep("grid_df_res", geoparsingTableNames)])
geoparsingTableNames <- unique(geoparsingTableNames)

## Loop through all the tables and write to the database
for(i in 1:length(geoparsingTableNames)){
  # identify latest version of the file
  files <- geoparsingFiles[grep(geoparsingTableNames[i], geoparsingFiles)]
  fileVersions <- substr(files, nchar(geoparsingTableNames[i])+1, nchar(geoparsingTableNames[i])+1)
  if("." %in% fileVersions){
    fileVersions[which(fileVersions == ".")] <- 0
  }
  fileVersions <- as.numeric(fileVersions)
  file <- files[which.max(fileVersions)]
  
  # Read in file
  df <- readr::read_csv(file = file.path(geoparsingDir, file), show_col_types = FALSE)
  # Have to add this because some column names are duplicates (one is upper, the other is lower case)
  if(geoparsingTableNames[i] == "shp_df_natural-earth-shapes"){
    colnames(df) <- make.unique(tolower(colnames(df)), sep = "_")
  }
  # Write to database
  dbWriteTable(conn=dbcon, name=geoparsingTableNames[i], value=df, append=FALSE, overwrite = TRUE)
}

src_dbi(dbcon)

DBI::dbDisconnect(dbcon)
```


```{r additional data cleaning of false associations between country matches for v3}
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, "all_tables_v3.sqlite"), create=FALSE)

example_ids <- c(35143, 527)

geoparsed_text <- tbl(dbcon, "geoparsed_text") %>%
  filter(analysis_id %in% example_ids) %>%
  collect()

shp_df_matches <- tbl(dbcon, "geoparsed-text_shp_df_matches") %>%
  filter(analysis_id %in% example_ids) %>%
  collect()

grid_df <- tbl(dbcon, "grid_df_res2.5") %>%
  collect() 

grid_df2 <- grid_df %>% right_join(shp_df_matches, by = "grid_df_id")


## quick plot
plot(grid_df$LON, grid_df$LAT)

# So the geoparsing and matching to shapefiles looks fine, but its the intersection between shpfile polygons to grid ids that looks to be the problem. Try in R? The relevant shp_id is 31



##  Recalculate shp_id to grid_df_id matching in R -----------
shp_grid_df <- read.csv("C:\\Users\\deviv\\R-working-folder\\spacy_example\\outputs\\shp_grid_df.csv")

countries <- map_data("world")

shp_grid_df %>%
  filter(shpfile_id == 31) %>%
  left_join(grid_df, by = "grid_df_id") %>%
  View() 

shp_grid_df %>%
  filter(shpfile_id == 31) %>%
  left_join(grid_df, by = "grid_df_id") %>%
  ggplot(aes(LON, LAT))+
  geom_point(col="red")+
  geom_polygon(data = countries, aes(long, lat, group=group))

# The version calculated in my spacy script seems good, so use this and make the sqlite file version 2

shp_df_matches <- tbl(dbcon, "geoparsed-text_shp_df_matches") %>% collect()

# Separate where there is and isn't a shp_id
shp_df_matches_noID <- shp_df_matches %>% filter(is.na(shp_id))
shp_df_matches_ID <- shp_df_matches %>% filter(!is.na(shp_id))

# Get unique combinations of analysis id, shp id, and place to rematch to grid cells
shp_df_matches_ID_unique <- shp_df_matches_ID[c("analysis_id","shp_id","place")]
shp_df_matches_ID_unique <- shp_df_matches_ID_unique[!duplicated(shp_df_matches_ID_unique),]

# for each shp_id, add new rows for all the relevant grid cells
shp_df_matches_ID_unique <- merge(shp_df_matches_ID_unique, shp_grid_df %>% rename(shp_id = "shpfile_id"),
                                  by = "shp_id", all.x = TRUE, all.y = FALSE)


# Double check again shp_id 31 (the UK) -- yes
shp_df_matches_ID_unique %>%
  filter(shp_id == 31) %>%
  left_join(grid_df, by = "grid_df_id") %>%
  ggplot(aes(LON, LAT))+
  geom_point(col="red")+
  geom_polygon(data = countries, aes(long, lat, group=group))



# Calculate the grid cell weight
## For each unique document and place, calculate the 1/the number of grid cells occupied by that place to get the grid cell weight
shp_df_matches_ID_unique <- shp_df_matches_ID_unique %>%
  group_by(analysis_id, shp_id) %>%
  mutate(cell_weight = 1/n())

# noticed that where there was no shp_id, cell weight was NA so recalculate grouping by place
shp_df_matches_noID <- shp_df_matches_noID %>%
  group_by(analysis_id, place) %>%
  mutate(cell_weight = 1/n())

## Rejoin new shp_id to grid matches with those rows that didn't have a shp_id
shp_df_matches2 <- rbind(shp_df_matches_ID_unique, shp_df_matches_noID)


## Recalculate grid sums
shp_df_sum = shp_df_matches2 %>%
  group_by(grid_df_id) %>%
  summarise(n_articles = n(), n_articles_weighted = sum(cell_weight, na.rm=TRUE)) %>%
  right_join(grid_df, by = "grid_df_id")

# quick plot
shp_df_sum %>%
  ggplot()+
  geom_tile(aes(LON, LAT, fill = log(n_articles_weighted)))+
  scale_fill_viridis_c(option = "D")+
  geom_polygon(data = countries, aes(long, lat, group=group), fill="transparent")
  #coord_map(projection = "mercator")


DBI::dbDisconnect(dbcon)

```

```{r Add fixes to new sqlite database v4}
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, "all_tables_v4.sqlite"), create=FALSE)

# Write over old tables
dbWriteTable(conn=dbcon, name="geoparsed-text_shp_df_matches", value=shp_df_matches2, append=FALSE, overwrite = TRUE)

dbWriteTable(conn=dbcon, name="shp_grid_df", value=shp_grid_df, append=FALSE, overwrite = TRUE)

dbWriteTable(conn=dbcon, name="geoparsed-text_grid-sums", value=shp_df_sum, append=FALSE, overwrite = TRUE)


DBI::dbDisconnect(dbcon)
```


## Clean name matching errors - v5

This section will clean the bad place matches from the geoparsing tables.

Steps to clean:
1. Remove acronym matches where the word matched is all capitalized letters, except UK and US
2. Remove text between copyright symbol and next period (get rid of publisher matching)
3. Remove land matches x distance from the coast when the spatial scale of the match is sub-national
4. Remove affiliation text from abstract, and any resulting word matches
5. Remove word matches that are a complete word in the English language
6. Weight match by country_conf value (0.6 - 1)** -> consider removing low values using cluster analysis altogether


Notes on no. 3: 
There are several definitions we can use to identify coastal land (from least to most conservative):
a. Within 100 km from the land-sea interface (WRI 2000,Burke et al. 2001)
b. Landward area contained within 100 m elevation of sea level (Nicholls and Small 2002)
c. Landward area contained within the 200 m land elevation contour ((Land-ocean interaction in the coastal zone programme -- now Future Earth Coasts)[https://www.researchgate.net/publication/226867603_The_Coastal_Zone_-_a_Domain_of_Global_Interactions], Pernettaand Milliman 1995). This is designed to capture the area "extending from the coastal plains to the outer edge of the continental shelves, approximately matching the region that has been alternatively flooded and exposed during the sea level fluctuations of the late Quaternary period‚Äù(Holligan and de Boois 1993). 

I suggest to take the conservative approach of option "c" and do landward area within 200 m land elevation contour. That said, if it is too complicated/time consuming, then option "a" looks easier -- we could make the argument that we extend it to 200 km from coast because given sea level rise locations further inland may be concerned with coastal research? 

*Estimating error rate*

Another approach is to create a lookup table of unique word and place_name matches and double check manually -- this can then be merged into the geoparsed_text table to indicate whether a match is TRUE, FALSE, or ambiguous. This will allow us to estimate the likely error rate, which can be stratified by country. This information can be used to:
a) clean the data by completely eliminating matches that are FALSE
b) estimate "uncertainty" for each country using the "ambiguous" classification -- perhaps this information can be displayed alongside the data plotted in the map via stippling etc. This will likely apply to countries like the USA, where records like "south beach" are attributed.



```{r load data}
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, "all_tables_v4.sqlite"), create=FALSE)

geoparsedText <- tbl(dbcon, "geoparsed_text") |>  collect()
shp_df_matches <- tbl(dbcon, "geoparsed-text_shp_df_matches") 
shp_df_sum <- tbl(dbcon, "geoparsed-text_grid-sums")
uniquerefs <- tbl(dbcon, "uniquerefs")

```


```{r steps for cleaning}

### Note N¬∞1: Capital letter filtering

  ## Extract world cities and countries names
  library(maps)
  cities <- as.data.frame(world.cities) 
  
  ## Filter rows if the word match in capital letter
  word_in_capital_letters_TRUE <- tbl(dbcon, "geoparsed_text") |>  
    dplyr::select(analysis_id, word, country_predicted, place_name) |> 
    collect() |> 
    # Filter row with a matching pattern in CAPITAL letters
    filter(stringr::str_detect(word, pattern = "^[[:upper:][:space:]]+$")) |> 
    mutate(word_title = stringr::str_to_title(word),
           # New column to identify CAPITAL letters patterns that are country names
           id_country = countrycode(sourcevar   = word,
                                    origin      = "country.name",
                                    destination = "country.name"),
           # New column to identify CAPITAL letters patterns that are city names
           city_T_F  = word_title %in% cities$name) |> 
    # Filter rows whose the column word corresponds to a country OR city name
    filter(!is.na(id_country) | city_T_F == TRUE)

```

```{r visualize data structure}
# uniqueWordPlaceMatchesCountryPred <- geoparsedText %>% select(word, place_name) %>% distinct(word, place_name) 

uniqueWordPlaceMatchesCountryPred <- geoparsedText %>% 
  select(word, place_name, analysis_id, country_predicted) %>% 
  group_by(word, place_name, country_predicted) |> 
  summarise(n_articles = n()) |> 
  collect()

nrow(uniqueWordPlaceMatchesCountryPred) # 8261



# Filter to those for which we actually have matches to the natural earth database
# sum(uniqueWordPlaceMatches$place_name %in% )


# uniqueWordPlaceMatches$n_articles <- rep(NA, nrow(uniqueWordPlaceMatches))
# for(i in 1:nrow(uniqueWordPlaceMatches)){
#   uniqueWordPlaceMatches$n_articles[i] <- sum(geoparsedText$word == uniqueWordPlaceMatches$word[i] &
#           geoparsedText$place_name == uniqueWordPlaceMatches$place_name[i])
# }

# Check to see if the tidyverse code gave the same results as the R base code (IT DOES)
# test1 <- geoparsedText %>% 
#   select(word, place_name, analysis_id) %>% 
#   group_by(word, place_name,) |> 
#   summarise(n_articles = n()) |> 
#   collect()
# quantile(test1$n_articles, probs = c(0,0.75, 0.8, 0.85, 0.9, 0.95, 1))



quantile(uniqueWordPlaceMatchesCountryPred$n_articles, probs = c(0,0.75, 0.8, 0.85, 0.9, 0.95, 1))

sum(uniqueWordPlaceMatchesCountryPred$n_articles[17 <= uniqueWordPlaceMatchesCountryPred$n_articles])
sum(uniqueWordPlaceMatchesCountryPred$n_articles[uniqueWordPlaceMatchesCountryPred$n_articles < 17])


highFreqMatches <- uniqueWordPlaceMatchesCountryPred %>%
  filter(17 <= n_articles) %>%
  arrange(desc(n_articles)) 

nrow(highFreqMatches) # 437 articles to check

write.csv(highFreqMatches, file = here::here("data/geoparsing/geoparsing_wordPlaceMatches95quantile.csv"))


shp_df <- tbl(dbcon, "shp_df_natural-earth-shapes") %>%
  select(name,name_long, name_en,name_local, abbrev, formal_en,adm0_a3_us, continent,region_un,region, subregion,region_wb,gn_name, shpfile_id) %>%
  collect()

shp_df_names <- apply(shp_df, 1, function(x) paste(x, collapse = " "))

shp_df[grep("Everglade", shp_df_names, ignore.case = TRUE),] %>% View
shp_df[grep("africa", shp_df_names, ignore.case = TRUE),c("name","shpfile_id")] %>% View

temp <- shp_df[grep("eastern africa", shp_df$subregion, ignore.case = TRUE),]
temp <- temp %>% filter(subregion %in% c("Melanesia","Micronesia","Polynesia"))
paste(temp$name, collapse = "; ")
paste(temp$shpfile_id, collapse = "; ")

shp_df_matches[grep("everglade", shp_df_matches$place, ignore.case = TRUE),] %>%
  filter(!is.na(shp_id)) %>%
  distinct(place, shp_id) %>%
  select(place, shp_id) %>%
  View()


# After cleaning, read in data and visualize
highFreqMatches_clean <- read.csv(here::here("data/geoparsing/geoparsing_wordPlaceMatches95quantile_cleaned.csv"))

highFreqMatches_clean %>%
  group_by(true_match, ambiguous) %>%
  summarise(n_articles = sum(n_articles))

sum(highFreqMatches_clean$shp_id_correct!= "")
sum(highFreqMatches_clean$true_match == FALSE)
sum(highFreqMatches_clean$n_articles[highFreqMatches_clean$shp_id_correct!= ""])

highFreqMatches_clean %>%
  group_by(true_match, ambiguous) %>%
  summarise(n_articles = sum(n_articles)) %>%
  reshape2::melt(value.var = "n_articles") %>%
  ggplot(aes(x=true_match, y=value, fill = ambiguous))+
  geom_col(position = "stack")+
  theme_bw()

```


```{r estimate sample size needed to detect error}

## Step 1: Power analysis
# This analysis can be used to estimate what sample size (ie the number of records that need to be checked) in order to estimate the proportion of error for each country

n_mismatch <- pwr::pwr.p.test(h = 0.10, # estimate
           sig.level = 0.05,
           power = 0.80,
           alternative = "greater")

# The total number of rows needed to be screened in order to quantify/detect error needs to be multiplied by number of countries
formatC(ceiling(n_mismatch$n)*length(unique(geoparsedText$country_predicted)), big.mark = ",", format="d")
# 139,275

# This number is  >> than the total number of rows, so will need to screen/check all the rows in the data

```

```{r Add fixes to new sqlite database v5}
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, "all_tables_v5.sqlite"), create=FALSE)

# Write over old tables
dbWriteTable(conn=dbcon, name="geoparsed_text", value=geoparsedText, append=FALSE, overwrite = TRUE)

dbWriteTable(conn=dbcon, name="geoparsed-text_shp_df_matches", value=shp_df_matches2, append=FALSE, overwrite = TRUE)

dbWriteTable(conn=dbcon, name="geoparsed-text_grid-sums", value=shp_df_sum, append=FALSE, overwrite = TRUE)


DBI::dbDisconnect(dbcon)
```




```{r disconnect from database}
DBI::dbDisconnect(dbcon)
```

# Combine ORO_type predictions into long format

the predictions for the different ORO types are grouped into separate tables based on which branch they are relevant for. So make sure to combine them all, and order them along the x axis so that they are grouped by ORO branch (i.e. Mitigation vs Natural vs Societal) and not alphabetically. To faciliate this, create a new data table and add it to the sqlite database

```{r oro_type_long format}
# Connect to latest version of the database
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, latestVersion), create=FALSE)


# Format the dataframes so they're in long format, 
# with ID columns for analysis_id and oro_branch and oro_type, prediction boundary, 
# and value column for prediction
branchNames <- c("mitigation","nature","societal")
branchNamesFull <- c("Mitigation","Natural resilience", "Societal adaptation")
predOROAnyList <- list() # empty list to store results

for(i in 1:length(branchNames)){
  tempTbl <- tbl(dbcon, paste("pred_oro_any", branchNames[i], sep = "_")) %>%
    collect()
  # only look at columns with analysis id and mean predictions
  #meanCols <- colnames(tempTbl)[grep("mean", colnames(tempTbl))]
  #tempTbl <- tempTbl[,c("analysis_id", meanCols)]
  
  # rename columns 
  # colNames <- gsub("\\ -.*","",meanCols)
  # colNames <- gsub("oro_any.","", colNames)
  # colNames <- gsub("M_","", colNames)
  # colNames <- gsub("N_","", colNames)
  # colNames <- gsub("SA_","", colNames)
  # colnames(tempTbl)[which(colnames(tempTbl) %in% meanCols)] <- colNames
  colNames <- colnames(tempTbl)
  colNames[1] <- "analysis_id"
  colNames <- gsub("oro_any.","",colNames)
  colNames <- gsub("M_","", colNames)
  colNames <- gsub("N_","", colNames)
  colNames <- gsub("SA_","", colNames)
  colNames <- gsub(" - ","_", colNames)
  colNames <- gsub("_prediction","", colNames)
  colNames <- gsub("_pred","", colNames)
  colnames(tempTbl) <- colNames
  
  # Melt and save data frame
  tempTbl$oro_branch = paste(branchNamesFull[i])
  predOROAnyList[[i]] <- tempTbl
  #tempTbl <- reshape2::melt(tempTbl, variable.name = "oro_type", value.name = "mean_prediction", 
  #                         id.vars = c("analysis_id","oro_branch"))
  # if(b == 1){
  #   oro_type_long <- tempTbl
  # }else{
  #   oro_type_long <- rbind(oro_type_long, tempTbl)
  # }
}


## Make a dataframe of the predictions
for(i in 1:length(predOROAnyList)){
  temp <- predOROAnyList[[i]]
  boundaries <- c("upper","mean","lower")
  for(b in 1:length(boundaries)){
    temp_b <- cbind(temp[,1], temp$oro_branch, temp[grep(boundaries[b], colnames(temp))])
    colnames(temp_b)[2]<- "oro_branch"
    colnames(temp_b) <- gsub(paste0("_",boundaries[b]),"", colnames(temp_b))
    temp_b <- reshape2::melt(temp_b, variable.name = "oro_type", value.name = boundaries[b], 
                             id.vars = c("analysis_id","oro_branch"))
    if(b==1){
      temp_melt <- temp_b
    }else{
      temp_melt <- merge(temp_melt, temp_b, by=c("analysis_id","oro_branch","oro_type"))
    }
  }
  if(i==1)
    pred_oro_type_long <- temp_melt
  else
    pred_oro_type_long <- rbind(pred_oro_type_long, temp_melt)
}
rm(predOROAnyList)



# Format the dataframe by factoring variables
pred_oro_type_long <- pred_oro_type_long %>%
  filter(!(oro_type %in% c("Protection", "Restoration"))) %>%
  mutate(
    oro_branch = factor(oro_branch, levels = branchNamesFull),
    oro_type = factor(oro_type,
                      levels = c("Renewables","Increase_efficiency", "CO2_removal_or_storage",
                                 "Human_assisted_evolution","Conservation",
                                 "Built_infrastructure_and_technology","Socioinstitutional"),
                      labels = c("Marine renewable energy","Increase efficiency", "CO2 removal or storage",
                                 "Human assisted evolution","Conservation",
                                 "Built infrastructure & technology","Socio-institutional"))
  )


## Write in to sqlite database
dbWriteTable(conn=dbcon, name="pred_oro_type_long", value=pred_oro_type_long, append=FALSE, overwrite = FALSE)


DBI::dbDisconnect(dbcon)
```


# Write test list after coding (n=86)

```{r write test list}
# From analysis file: 08
# data load
# get screen results after coding
# Connect to latest version of the database
dbcon <- RSQLite::dbConnect(RSQLite::SQLite(), file.path(sqliteDir, latestVersion), create=FALSE)

seen_screen <- tbl(dbcon, "seen_screen")
dedups <- tbl(dbcon, "uniquerefs")

test_list_final <- seen_screen %>%
  filter(sample_screen == "test list"& include_screen == 1) %>%
  left_join(dedups %>% select(analysis_id, year, author, doi), by= "analysis_id") %>%
  select(analysis_id, title, abstract, keywords, year, author, doi) %>%
  collect()

nrow(test_list_final) # 86

dbDisconnect(dbcon)



writexl::write_xlsx(test_list_final, here::here("outputs/test_list.xlsx"))


```



